{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b06d6b-5faf-4953-a610-0c77014b5dec",
   "metadata": {},
   "source": [
    "<h2>Setting up</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2da1ee3-0cdd-4a35-a8c2-6ebedd482f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OS Path module contains some useful functions on pathnames. The path parameters are either strings or bytes. These functions here are used for different purposes such as for merging, normalizing, and retrieving path names in Python\n",
    "#The purpose of glob is to match file and directory names using wildcards to find files on a filesystem\n",
    "import os\n",
    "import torch\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d66922e2-bbf3-4c02-9cfe-6c04744bd26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display images\n",
    "from IPython.display import Image,display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d053d226-ceed-4d5d-aa42-4ddaf5c0b179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.70  Python-3.13.1 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1650 with Max-Q Design, 4096MiB)\n",
      "Setup complete  (8 CPUs, 31.6 GB RAM, 252.0/475.5 GB disk)\n"
     ]
    }
   ],
   "source": [
    "#Checking whether YOLOv8 is installed and working\n",
    "import ultralytics as uly\n",
    "uly.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6befaeb8-901d-4457-bda6-073deedb02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Ultralytics Setting\n",
    "from ultralytics import settings\n",
    "settings.update({\"wandb\": True,\n",
    "                 \"tensorboard\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e22df89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\env\\\\Lib\\\\site-packages\\\\torch']\n"
     ]
    }
   ],
   "source": [
    "#check Touch is working\n",
    "torch.cuda.get_arch_list()\n",
    "print(torch.__path__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "112c2a9b-9cc6-49af-8106-ef1609d2a505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Check for NVIDA CUDA is detected\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0cd2e30-b3e2-43ed-8bce-97ca6e79e72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if the model is using a GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  \n",
    "    print(\"Using GPU:\", device) \n",
    "else:\n",
    "    print(\"No GPU available, using CPU\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeda7f0-3ae1-4ccc-b596-149ae7c91d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\n"
     ]
    }
   ],
   "source": [
    "#Get into Project directory\n",
    "HOME=os.getcwd()\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d632b4f2",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f19576a",
   "metadata": {},
   "source": [
    "<h2>Feature Extraction using trained model YOLOv8n<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a924ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run a sample Test using YOLOv8n before fine tuning\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "\n",
    "img_path='data_for_testing_set1'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "output_dir = \"datasets/result_pretrained\"  # Choose your folder name\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#Testing Model 1 - Transfer Learning on Datasetvs1\n",
    "model_best = YOLO('yolov8n.pt')\n",
    "result = model_best(img_path, conf=0.5, iou=0.6)\n",
    "\n",
    "# Visualize the results\n",
    "for i, r in enumerate(result):\n",
    "    print(r)\n",
    "    # Plot results image\n",
    "    im_bgr = r.plot()  # BGR-order numpy array\n",
    "    im_rgb = Image.fromarray(im_bgr[..., ::-1])  # RGB-order PIL image\n",
    "    # Save results to disk\n",
    "    r.save(filename=os.path.join(output_dir, f\"results{i}.jpg\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451951c0",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6475646f",
   "metadata": {},
   "source": [
    "<h2>Fine Turning: Train Model 1x using YOLOv8s</h2>\n",
    "<h3>Dataset 1 (YOLOv8)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "887d7284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1\n"
     ]
    }
   ],
   "source": [
    "target_dir = os.path.join(HOME, 'datasets', 'datasetvs1')\n",
    "os.chdir(target_dir)\n",
    "print(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1f7974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check data loading before training\n",
    "import yaml\n",
    "\n",
    "with open(\"data.yaml\", 'r') as f:\n",
    "    data = yaml.safe_load(f)\n",
    "\n",
    "print(data)  # Print the loaded data to check for errors\n",
    "\n",
    "# Verify paths:\n",
    "print(\"Train path:\", data['train'])\n",
    "print(\"Val path:\", data['val'])\n",
    "print(\"Test path:\", data['test'])\n",
    "print(\"Number of classes:\", data['nc'])\n",
    "print(\"Class names:\", data['names'])\n",
    "\n",
    "# Check if the paths exist (optional, but helpful):\n",
    "import os\n",
    "print(\"Train path exists:\", os.path.exists(data['train']))\n",
    "print(\"Val path exists:\", os.path.exists(data['val']))\n",
    "print(\"Test path exists:\", os.path.exists(data['test']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a2fcc8",
   "metadata": {},
   "source": [
    "<b>Train Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcaca65-7fce-4aa1-af8b-a7093b3e72fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.74 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.70  Python-3.13.1 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1650 with Max-Q Design, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=data.yaml, epochs=30, time=None, patience=20, batch=16, imgsz=640, save=True, save_period=1, cache=False, device=0, workers=8, project=model1, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=model1\\train2\n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2117983  ultralytics.nn.modules.head.Detect           [5, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11,137,535 parameters, 11,137,519 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks failed . AMP training on NVIDIA GeForce GTX 1650 with Max-Q Design GPU may cause NaN losses or zero-mAP results, so AMP will be disabled during training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1\\train\\labels.cache... 225 images, 0 backgrounds, 0 corrupt: 100%|██████████| 225/225 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1\\valid\\labels.cache... 45 images, 0 backgrounds, 0 corrupt: 100%|██████████| 45/45 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to model1\\train2\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mmodel1\\train2\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/30      6.83G     0.7186      2.456      1.121          5        640: 100%|██████████| 15/15 [07:07<00:00, 28.50s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:27<00:00, 13.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.541      0.888      0.879      0.781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/30      6.91G     0.5518      1.019      1.003          5        640: 100%|██████████| 15/15 [07:10<00:00, 28.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:31<00:00, 15.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.906       0.97      0.989      0.911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/30      6.74G     0.5247      0.803     0.9552          5        640: 100%|██████████| 15/15 [08:34<00:00, 34.31s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:52<00:00, 26.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.839       0.85      0.956      0.859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/30      6.93G     0.5691     0.9379     0.9789          4        640: 100%|██████████| 15/15 [06:46<00:00, 27.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:32<00:00, 16.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.744      0.861      0.733      0.639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/30      6.93G     0.5184     0.6451     0.9517          4        640: 100%|██████████| 15/15 [06:49<00:00, 27.29s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:36<00:00, 18.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.905      0.824      0.867      0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/30      6.93G     0.5597     0.6199     0.9856          6        640: 100%|██████████| 15/15 [06:56<00:00, 27.74s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:32<00:00, 16.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.778      0.907      0.916       0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/30      6.68G     0.5316     0.5761     0.9677          4        640: 100%|██████████| 15/15 [06:30<00:00, 26.03s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:27<00:00, 13.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.837      0.914      0.923      0.811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/30      6.73G     0.5979     0.5996      0.997          4        640: 100%|██████████| 15/15 [06:21<00:00, 25.41s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:23<00:00, 11.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.911      0.897      0.973      0.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/30      6.69G     0.5404     0.4964     0.9789          4        640: 100%|██████████| 15/15 [07:37<00:00, 30.48s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:57<00:00, 28.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.944      0.878      0.968      0.861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/30      6.69G     0.5678     0.4852      1.027          2        640: 100%|██████████| 15/15 [11:36<00:00, 46.47s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:53<00:00, 26.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111       0.96      0.984      0.993      0.901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/30      6.68G     0.5213      0.461     0.9672          6        640: 100%|██████████| 15/15 [11:06<00:00, 44.43s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:35<00:00, 17.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111        0.9      0.934      0.967      0.876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/30      6.73G     0.5245     0.5306     0.9686          4        640: 100%|██████████| 15/15 [09:04<00:00, 36.27s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:54<00:00, 27.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.948       0.95      0.966       0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/30      6.93G     0.5045     0.4978      0.965          6        640: 100%|██████████| 15/15 [09:12<00:00, 36.84s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:49<00:00, 24.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.983          1      0.995      0.913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/30       6.7G      0.494     0.4784     0.9559          5        640: 100%|██████████| 15/15 [06:37<00:00, 26.52s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:31<00:00, 15.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.995      0.986      0.995      0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/30      6.68G     0.5035      0.466     0.9317          5        640: 100%|██████████| 15/15 [07:35<00:00, 30.36s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:29<00:00, 14.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.983      0.978      0.994      0.919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/30      6.73G     0.5133     0.5094     0.9739          2        640: 100%|██████████| 15/15 [07:17<00:00, 29.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:30<00:00, 15.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.987      0.993      0.995       0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/30      6.93G     0.4707      1.504     0.8996          0        640: 100%|██████████| 15/15 [07:04<00:00, 28.29s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:37<00:00, 18.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.983      0.997      0.995       0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/30       6.7G     0.4933     0.3919     0.9486          8        640: 100%|██████████| 15/15 [07:07<00:00, 28.51s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:38<00:00, 19.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.981      0.997      0.995      0.918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/30      6.68G     0.4979     0.4206      0.957          2        640: 100%|██████████| 15/15 [07:54<00:00, 31.63s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:31<00:00, 15.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.985          1      0.995      0.922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/30      6.73G     0.4994     0.3948     0.9547          8        640: 100%|██████████| 15/15 [13:35<00:00, 54.39s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:29<00:00, 14.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.983      0.999      0.994      0.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/30      6.93G     0.3689     0.2971     0.8617          4        640: 100%|██████████| 15/15 [11:19<00:00, 45.29s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [01:01<00:00, 30.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.983          1      0.995      0.927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/30       6.7G     0.3472     0.4724     0.8466          1        640: 100%|██████████| 15/15 [10:16<00:00, 41.07s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:54<00:00, 27.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.995          1      0.995       0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/30      6.69G     0.3642     0.4976     0.8671          3        640: 100%|██████████| 15/15 [10:40<00:00, 42.72s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:46<00:00, 23.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.996          1      0.995      0.941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/30      6.73G     0.3523     0.4097      0.855          3        640: 100%|██████████| 15/15 [14:55<00:00, 59.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:28<00:00, 14.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.995          1      0.995      0.943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/30      6.69G     0.3414     0.3528      0.861          4        640: 100%|██████████| 15/15 [19:33<00:00, 78.25s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [01:26<00:00, 43.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.996          1      0.995      0.951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/30      6.69G     0.3249     0.2602     0.8421          4        640: 100%|██████████| 15/15 [13:58<00:00, 55.89s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:45<00:00, 22.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.996          1      0.995      0.945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/30      6.68G     0.3435     0.2554     0.8753          1        640: 100%|██████████| 15/15 [17:33<00:00, 70.22s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:56<00:00, 28.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.995          1      0.995       0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/30      6.73G     0.3294     0.2425     0.8533          5        640: 100%|██████████| 15/15 [13:35<00:00, 54.38s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [01:03<00:00, 31.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.996          1      0.995      0.946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/30      6.93G     0.3138     0.3029     0.8452          3        640: 100%|██████████| 15/15 [10:24<00:00, 41.66s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [01:15<00:00, 37.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.996          1      0.995      0.948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/30       6.7G     0.3222     0.2851     0.8459          2        640: 100%|██████████| 15/15 [18:13<00:00, 72.87s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:25<00:00, 12.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.996          1      0.995      0.951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "30 epochs completed in 5.430 hours.\n",
      "Optimizer stripped from model1\\train2\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from model1\\train2\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating model1\\train2\\weights\\best.pt...\n",
      "Ultralytics 8.3.70  Python-3.13.1 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1650 with Max-Q Design, 4096MiB)\n",
      "Model summary (fused): 168 layers, 11,127,519 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.996          1      0.995      0.951\n",
      "                 apple         19         40      0.997          1      0.995      0.938\n",
      "                banana         11         12      0.997          1      0.995      0.962\n",
      "                  kiwi         10         18      0.994          1      0.995      0.963\n",
      "                  pear         14         22      0.995          1      0.995      0.933\n",
      "             starfruit         13         19      0.996          1      0.995       0.96\n",
      "Speed: 4.2ms preprocess, 161.2ms inference, 0.0ms loss, 2.4ms postprocess per image\n",
      "Results saved to \u001b[1mmodel1\\train2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "\"\"\"Model 1a using dataset 1 (YOLOv8)\n",
    "model = YOLO('yolov8s.pt')\n",
    "result = model.train(data=\"data.yaml\",\n",
    "                    epochs=30,\n",
    "                    save_period=1,\n",
    "                    batch=16,\n",
    "                    device=0,\n",
    "                    project='model1',\n",
    "                    plots=True,\n",
    "                    patience=10,\n",
    "                    momentum=0.937,\n",
    "                    weight_decay=0.0005,\n",
    "                    imgsz=640)\n",
    "\"\"\"\n",
    "#Model 1b using dataset 1 (YOLOv8)\n",
    "model = YOLO('yolov8s.pt')\n",
    "result = model.train(data=\"data.yaml\",\n",
    "                    epochs=30,\n",
    "                    save_period=1,\n",
    "                    batch=16,\n",
    "                    device=0,\n",
    "                    project='model1',\n",
    "                    plots=True,\n",
    "                    patience=20,\n",
    "                    momentum=0.937,\n",
    "                    weight_decay=0.0005,\n",
    "                    imgsz=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faa1081",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea995fb",
   "metadata": {},
   "source": [
    "<h2>Fine Turning: Train the Model 1x using YOLOv11n</h2>\n",
    "<h3>Dataset 1 (Format YOLOv8) & Dataset 1 (Format YOLOv11)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1479712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\n"
     ]
    }
   ],
   "source": [
    "target_dir = os.path.join(HOME, 'datasets', 'datasetvs1_yolvo11')\n",
    "os.chdir(target_dir)\n",
    "print(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77801863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 'C:/Users/wensi/Desktop/machine_learning/object_detector/ITC110_assignment_submission/datasets/datasetvs1_yolvo11/train/images', 'val': 'C:/Users/wensi/Desktop/machine_learning/object_detector/ITC110_assignment_submission/datasets/datasetvs1_yolvo11/valid/images', 'test': 'C:/Users/wensi/Desktop/machine_learning/object_detector/ITC110_assignment_submission/datasets/datasetvs1_yolvo11/test/images', 'nc': 5, 'names': ['apple', 'banana', 'kiwi', 'pear', 'starfruit'], 'roboflow': {'workspace': 'itc107-assignment-1-79crc', 'project': 'nyp-itc110-project', 'version': 1, 'license': 'CC BY 4.0', 'url': 'https://universe.roboflow.com/itc107-assignment-1-79crc/nyp-itc110-project/dataset/1'}}\n",
      "Train path: C:/Users/wensi/Desktop/machine_learning/object_detector/ITC110_assignment_submission/datasets/datasetvs1_yolvo11/train/images\n",
      "Val path: C:/Users/wensi/Desktop/machine_learning/object_detector/ITC110_assignment_submission/datasets/datasetvs1_yolvo11/valid/images\n",
      "Test path: C:/Users/wensi/Desktop/machine_learning/object_detector/ITC110_assignment_submission/datasets/datasetvs1_yolvo11/test/images\n",
      "Number of classes: 5\n",
      "Class names: ['apple', 'banana', 'kiwi', 'pear', 'starfruit']\n",
      "Train path exists: True\n",
      "Val path exists: True\n",
      "Test path exists: True\n"
     ]
    }
   ],
   "source": [
    "#Check data loading before training\n",
    "import yaml\n",
    "\n",
    "with open(\"data.yaml\", 'r') as f:\n",
    "    data = yaml.safe_load(f)\n",
    "\n",
    "print(data)  # Print the loaded data to check for errors\n",
    "\n",
    "# Verify paths:\n",
    "print(\"Train path:\", data['train'])\n",
    "print(\"Val path:\", data['val'])\n",
    "print(\"Test path:\", data['test'])\n",
    "print(\"Number of classes:\", data['nc'])\n",
    "print(\"Class names:\", data['names'])\n",
    "\n",
    "# Check if the paths exist (optional, but helpful):\n",
    "import os\n",
    "print(\"Train path exists:\", os.path.exists(data['train']))\n",
    "print(\"Val path exists:\", os.path.exists(data['val']))\n",
    "print(\"Test path exists:\", os.path.exists(data['test']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee4adf",
   "metadata": {},
   "source": [
    "<b>Train Model 1x</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683c2f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.35M/5.35M [00:00<00:00, 27.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.77 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.70  Python-3.13.1 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1650 with Max-Q Design, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=data.yaml, epochs=30, time=None, patience=20, batch=16, imgsz=640, save=True, save_period=1, cache=True, device=0, workers=8, project=model1, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.99, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=model1\\train\n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    431647  ultralytics.nn.modules.head.Detect           [5, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,815 parameters, 2,590,799 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks failed . AMP training on NVIDIA GeForce GTX 1650 with Max-Q Design GPU may cause NaN losses or zero-mAP results, so AMP will be disabled during training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\train\\labels... 225 images, 0 backgrounds, 0 corrupt: 100%|██████████| 225/225 [00:00<00:00, 485.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\train\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\valid\\labels... 45 images, 0 backgrounds, 0 corrupt: 100%|██████████| 45/45 [00:00<00:00, 165.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\valid\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to model1\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.99' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mmodel1\\train\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/30      4.36G     0.6649      3.309      1.019          5        640: 100%|██████████| 15/15 [00:47<00:00,  3.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:11<00:00,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111     0.0406          1      0.435      0.379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/30       4.4G     0.6052      2.468      1.002          5        640: 100%|██████████| 15/15 [00:51<00:00,  3.41s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:10<00:00,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111     0.0265          1        0.7      0.595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/30      4.39G      0.613      1.634      1.008          5        640: 100%|██████████| 15/15 [00:47<00:00,  3.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:11<00:00,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111     0.0169          1      0.868      0.768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/30      4.42G     0.6082      1.334      1.006          4        640: 100%|██████████| 15/15 [00:58<00:00,  3.88s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:10<00:00,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.893      0.313      0.954      0.866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/30      4.41G     0.5606      1.067     0.9761          4        640: 100%|██████████| 15/15 [00:46<00:00,  3.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:09<00:00,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.848      0.667      0.976      0.872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/30      4.38G     0.5916      1.055     0.9925          6        640: 100%|██████████| 15/15 [01:12<00:00,  4.83s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:14<00:00,  7.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111       0.82      0.905      0.973      0.869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/30       4.4G     0.5689     0.9878     0.9735          4        640: 100%|██████████| 15/15 [00:32<00:00,  2.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:09<00:00,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.867      0.949      0.953      0.868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/30      4.42G     0.5576     0.9491     0.9682          4        640: 100%|██████████| 15/15 [00:39<00:00,  2.62s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:09<00:00,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111       0.96      0.965      0.991      0.903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/30       4.4G     0.5349     0.8519     0.9641          4        640: 100%|██████████| 15/15 [00:40<00:00,  2.71s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:09<00:00,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.849      0.914      0.959      0.887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/30       4.4G     0.5906     0.8816      1.044          2        640: 100%|██████████| 15/15 [00:42<00:00,  2.83s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:12<00:00,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111       0.87      0.878      0.968      0.885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/30       4.4G     0.5373     0.8126     0.9598          6        640: 100%|██████████| 15/15 [00:32<00:00,  2.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.966      0.974      0.989      0.904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/30      4.42G     0.5143      0.811     0.9594          4        640: 100%|██████████| 15/15 [00:44<00:00,  2.97s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:09<00:00,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.976          1      0.995      0.919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/30       4.4G     0.5036     0.7862      0.957          6        640: 100%|██████████| 15/15 [00:49<00:00,  3.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:09<00:00,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.984          1      0.995       0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/30       4.4G     0.5091     0.7762     0.9518          5        640: 100%|██████████| 15/15 [00:44<00:00,  2.93s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:11<00:00,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.971      0.999      0.989      0.915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/30      4.38G     0.4857     0.7187     0.9156          5        640: 100%|██████████| 15/15 [00:45<00:00,  3.02s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:11<00:00,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.994      0.996      0.995      0.923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/30       4.4G     0.5248      0.878     0.9729          2        640: 100%|██████████| 15/15 [00:43<00:00,  2.87s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111       0.99          1      0.995      0.928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/30      4.39G     0.4689      1.221     0.8852          0        640: 100%|██████████| 15/15 [00:37<00:00,  2.52s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.994          1      0.995      0.935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/30      4.41G     0.4833     0.7031     0.9296          8        640: 100%|██████████| 15/15 [00:35<00:00,  2.34s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:15<00:00,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.926      0.975      0.995      0.939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/30       4.4G     0.4803     0.7601     0.9196          2        640: 100%|██████████| 15/15 [00:58<00:00,  3.90s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:11<00:00,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.996          1      0.995      0.939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/30      4.39G     0.4897     0.6523     0.9301          8        640: 100%|██████████| 15/15 [00:43<00:00,  2.93s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:09<00:00,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.995          1      0.995      0.927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/30       4.4G     0.3618     0.6738     0.8553          4        640: 100%|██████████| 15/15 [00:34<00:00,  2.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:12<00:00,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.898      0.974      0.993      0.934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/30      4.36G     0.3479     0.7415     0.8331          1        640: 100%|██████████| 15/15 [00:51<00:00,  3.42s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:15<00:00,  7.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.995          1      0.995      0.941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/30      4.39G     0.3548     0.6707     0.8504          3        640: 100%|██████████| 15/15 [02:01<00:00,  8.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:23<00:00, 11.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.995          1      0.995      0.942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/30      4.36G     0.3388     0.7384     0.8476          3        640: 100%|██████████| 15/15 [04:34<00:00, 18.33s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:54<00:00, 27.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.995          1      0.995      0.931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/30      4.36G     0.3396     0.6975     0.8497          4        640: 100%|██████████| 15/15 [01:37<00:00,  6.51s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:17<00:00,  8.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.994          1      0.995      0.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/30      4.36G     0.3316     0.6409     0.8325          4        640: 100%|██████████| 15/15 [01:26<00:00,  5.78s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:20<00:00, 10.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.997          1      0.995      0.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/30      4.39G     0.3276     0.6085     0.8483          1        640: 100%|██████████| 15/15 [02:25<00:00,  9.67s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:39<00:00, 19.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.996          1      0.995      0.947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/30      4.36G     0.3286     0.5645     0.8456          5        640: 100%|██████████| 15/15 [01:29<00:00,  5.98s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:14<00:00,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.996          1      0.995      0.947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/30       4.4G     0.3161     0.6257     0.8385          3        640: 100%|██████████| 15/15 [01:30<00:00,  6.07s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:20<00:00, 10.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.996          1      0.995      0.951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/30      4.36G     0.3259     0.6105      0.844          2        640: 100%|██████████| 15/15 [01:18<00:00,  5.22s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:15<00:00,  7.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.995          1      0.995       0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "30 epochs completed in 0.704 hours.\n",
      "Optimizer stripped from model1\\train\\weights\\last.pt, 5.5MB\n",
      "Optimizer stripped from model1\\train\\weights\\best.pt, 5.5MB\n",
      "\n",
      "Validating model1\\train\\weights\\best.pt...\n",
      "Ultralytics 8.3.70  Python-3.13.1 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1650 with Max-Q Design, 4096MiB)\n",
      "YOLO11n summary (fused): 238 layers, 2,583,127 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:04<00:00,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         45        111      0.996          1      0.995      0.951\n",
      "                 apple         19         40      0.997          1      0.995      0.933\n",
      "                banana         11         12      0.998          1      0.995      0.978\n",
      "                  kiwi         10         18      0.993          1      0.995      0.945\n",
      "                  pear         14         22      0.997          1      0.995      0.933\n",
      "             starfruit         13         19      0.994          1      0.995      0.965\n",
      "Speed: 1.5ms preprocess, 88.3ms inference, 0.0ms loss, 3.5ms postprocess per image\n",
      "Results saved to \u001b[1mmodel1\\train\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nmodel = YOLO(\\'yolo11n.pt\\')\\nresult = model.train(data=\"data.yaml\",\\n                    epochs=50,\\n                    save_period=1,\\n                    batch=16,\\n                    device=0,\\n                    project=\\'model1\\',\\n                    plots=True,\\n                    patience=20,\\n                    momentum=0.99,\\n                    weight_decay=0.0005,\\n                    optimizer=\\'Adam\\',\\n                    pretrained = \"True\",\\n                    cache=\\'True\\',\\n                    degrees=0.8, \\n                    scale=0.3, \\n                    bgr=0.5,\\n                    imgsz=640)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "#With Dataset 1 (YOLOv11) \n",
    "model = YOLO('yolo11n.pt')\n",
    "result = model.train(data=\"data.yaml\",\n",
    "                    epochs=30,\n",
    "                    save_period=1,\n",
    "                    batch=16,\n",
    "                    device=0,\n",
    "                    project='model1',\n",
    "                    plots=True,\n",
    "                    patience=20,\n",
    "                    momentum=0.99,\n",
    "                    weight_decay=0.0005,\n",
    "                    cache='True',\n",
    "                    imgsz=640)\n",
    "\n",
    "#Without data augmentation using using dataset 1 (YOLOv8)\n",
    "\"\"\"\n",
    "model = YOLO('yolo11n.pt')\n",
    "result = model.train(data=\"data.yaml\",\n",
    "                    epochs=50,\n",
    "                    save_period=1,\n",
    "                    batch=16,\n",
    "                    device=0,\n",
    "                    project='model1',\n",
    "                    plots=True,\n",
    "                    patience=20,\n",
    "                    momentum=0.99,\n",
    "                    weight_decay=0.0005,\n",
    "                    cache='True',\n",
    "                    imgsz=640)\n",
    "\"\"\"\n",
    "\n",
    "#With data augmentation and extra parameter using using dataset 1 (YOLOv8)\n",
    "\"\"\"\n",
    "model = YOLO('yolo11n.pt')\n",
    "result = model.train(data=\"data.yaml\",\n",
    "                    epochs=50,\n",
    "                    save_period=1,\n",
    "                    batch=16,\n",
    "                    device=0,\n",
    "                    project='model1',\n",
    "                    plots=True,\n",
    "                    patience=20,\n",
    "                    momentum=0.99,\n",
    "                    weight_decay=0.0005,\n",
    "                    optimizer='Adam',\n",
    "                    pretrained = \"True\",\n",
    "                    cache='True',\n",
    "                    degrees=0.8, \n",
    "                    scale=0.3, \n",
    "                    bgr=0.5,\n",
    "                    imgsz=640)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499c6308",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2a74df",
   "metadata": {},
   "source": [
    "<h1>Fine Turning: Train Model 3x using YOLOv8s & YOLOv11n</h2>\n",
    "<h3> Dataset 3 (YOLOv8) <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92078ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs3\n"
     ]
    }
   ],
   "source": [
    "target_dir = os.path.join(HOME, 'datasets', 'datasetvs3')\n",
    "os.chdir(target_dir)\n",
    "print(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "826d2e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 'C:/Users/wensi/Desktop/machine_learning/object_detector/ITC110_assignment_submission/datasets/datasetvs3/train/images', 'val': 'C:/Users/wensi/Desktop/machine_learning/object_detector/ITC110_assignment_submission/datasets/datasetvs3/valid/images', 'test': 'C:/Users/wensi/Desktop/machine_learning/object_detector/ITC110_assignment_submission/datasets/datasetvs3/test/images', 'nc': 5, 'names': ['apple', 'banana', 'kiwi', 'pear', 'starfruit']}\n",
      "Train path: C:/Users/wensi/Desktop/machine_learning/object_detector/ITC110_assignment_submission/datasets/datasetvs3/train/images\n",
      "Val path: C:/Users/wensi/Desktop/machine_learning/object_detector/ITC110_assignment_submission/datasets/datasetvs3/valid/images\n",
      "Test path: C:/Users/wensi/Desktop/machine_learning/object_detector/ITC110_assignment_submission/datasets/datasetvs3/test/images\n",
      "Number of classes: 5\n",
      "Class names: ['apple', 'banana', 'kiwi', 'pear', 'starfruit']\n",
      "Train path exists: True\n",
      "Val path exists: True\n",
      "Test path exists: True\n"
     ]
    }
   ],
   "source": [
    "#Check data loading before training\n",
    "import yaml\n",
    "\n",
    "with open(\"data.yaml\", 'r') as f:\n",
    "    data = yaml.safe_load(f)\n",
    "\n",
    "print(data)  # Print the loaded data to check for errors\n",
    "\n",
    "# Verify paths:\n",
    "print(\"Train path:\", data['train'])\n",
    "print(\"Val path:\", data['val'])\n",
    "print(\"Test path:\", data['test'])\n",
    "print(\"Number of classes:\", data['nc'])\n",
    "print(\"Class names:\", data['names'])\n",
    "\n",
    "# Check if the paths exist (optional, but helpful):\n",
    "import os\n",
    "print(\"Train path exists:\", os.path.exists(data['train']))\n",
    "print(\"Val path exists:\", os.path.exists(data['val']))\n",
    "print(\"Test path exists:\", os.path.exists(data['test']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa78a66",
   "metadata": {},
   "source": [
    "<b>Train Model 3x</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bfb8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.76 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.70  Python-3.13.1 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1650 with Max-Q Design, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=data.yaml, epochs=30, time=None, patience=20, batch=16, imgsz=640, save=True, save_period=1, cache=True, device=0, workers=8, project=model3, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=model3\\train2\n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    431647  ultralytics.nn.modules.head.Detect           [5, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,815 parameters, 2,590,799 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks failed . AMP training on NVIDIA GeForce GTX 1650 with Max-Q Design GPU may cause NaN losses or zero-mAP results, so AMP will be disabled during training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs3\\train\\labels.cache... 739 images, 80 backgrounds, 0 corrupt: 100%|██████████| 739/739 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs3\\valid\\labels.cache... 35 images, 2 backgrounds, 0 corrupt: 100%|██████████| 35/35 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to model3\\train2\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mmodel3\\train2\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/30      4.37G     0.7693      2.968      1.217         18        640: 100%|██████████| 47/47 [04:23<00:00,  5.61s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:12<00:00,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66     0.0237          1      0.611       0.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/30      4.51G     0.7844      1.917      1.224          9        640: 100%|██████████| 47/47 [04:59<00:00,  6.37s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:17<00:00,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.694      0.794      0.809      0.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/30       4.5G     0.7592      1.511      1.181         16        640: 100%|██████████| 47/47 [02:41<00:00,  3.44s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.708      0.833      0.868      0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/30       4.5G     0.7569      1.382      1.165          6        640: 100%|██████████| 47/47 [02:51<00:00,  3.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:11<00:00,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.625      0.751      0.738      0.604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/30      4.49G     0.7184      1.272      1.173          8        640: 100%|██████████| 47/47 [03:28<00:00,  4.43s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:09<00:00,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.727      0.756      0.796      0.548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/30       4.5G     0.6706      1.103      1.127          7        640: 100%|██████████| 47/47 [02:26<00:00,  3.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:11<00:00,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.851      0.983      0.969      0.877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/30      4.53G     0.6795      1.019      1.109          9        640: 100%|██████████| 47/47 [04:14<00:00,  5.41s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:09<00:00,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.887      0.901      0.923      0.834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/30      4.53G     0.6284     0.9414      1.085         14        640: 100%|██████████| 47/47 [03:13<00:00,  4.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:11<00:00,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.855      0.881      0.893      0.757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/30      4.52G     0.6257     0.8656      1.071         10        640: 100%|██████████| 47/47 [04:48<00:00,  6.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:09<00:00,  4.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.938      0.916      0.979      0.878\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/30      4.52G     0.6339     0.8517      1.082         12        640: 100%|██████████| 47/47 [03:07<00:00,  3.99s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:11<00:00,  5.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66       0.92      0.912      0.961      0.836\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/30      4.49G     0.6299     0.8154      1.078         10        640: 100%|██████████| 47/47 [04:06<00:00,  5.24s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:10<00:00,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.905      0.886       0.96      0.856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/30      4.48G     0.5931     0.7853      1.059          9        640: 100%|██████████| 47/47 [02:21<00:00,  3.02s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:11<00:00,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.907       0.98      0.975       0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/30      4.49G     0.5818     0.7475      1.057         10        640: 100%|██████████| 47/47 [04:28<00:00,  5.72s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:10<00:00,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.951      0.996      0.995      0.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/30      4.51G     0.5877     0.7259      1.059          8        640: 100%|██████████| 47/47 [03:16<00:00,  4.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:10<00:00,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.928      0.954      0.975      0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/30      4.47G     0.5671     0.7061       1.05         12        640: 100%|██████████| 47/47 [04:28<00:00,  5.71s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:14<00:00,  7.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66       0.95      0.964       0.98      0.855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/30       4.5G     0.5506     0.6575       1.03         15        640: 100%|██████████| 47/47 [08:00<00:00, 10.23s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:17<00:00,  8.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.976      0.959      0.989      0.909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/30      4.49G     0.5279     0.6372      1.014         16        640: 100%|██████████| 47/47 [06:38<00:00,  8.47s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:12<00:00,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.883      0.929      0.988      0.918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/30      4.51G     0.5088     0.6261      1.008         11        640: 100%|██████████| 47/47 [06:47<00:00,  8.66s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:19<00:00,  9.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.958      0.915      0.989      0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/30       4.5G     0.4933     0.6066      1.006          6        640: 100%|██████████| 47/47 [06:28<00:00,  8.27s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:15<00:00,  7.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.955      0.972      0.995      0.918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/30      4.52G     0.5402     0.6185      1.048          6        640: 100%|██████████| 47/47 [06:03<00:00,  7.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:14<00:00,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.981          1      0.995      0.907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/30      4.46G     0.4836      0.585      1.021          6        640: 100%|██████████| 47/47 [04:31<00:00,  5.77s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:11<00:00,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.972      0.954      0.995      0.903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/30      4.47G       0.47     0.5404      1.016          6        640: 100%|██████████| 47/47 [03:53<00:00,  4.96s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:11<00:00,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66       0.92          1      0.984      0.879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/30      4.47G     0.4629     0.5102      1.001          8        640: 100%|██████████| 47/47 [03:28<00:00,  4.44s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:10<00:00,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66       0.91      0.967       0.99      0.871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/30      4.47G     0.4386     0.4704     0.9802          6        640: 100%|██████████| 47/47 [02:21<00:00,  3.02s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.987      0.973      0.992      0.899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/30      4.46G     0.4304     0.4525     0.9815          2        640: 100%|██████████| 47/47 [03:35<00:00,  4.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:09<00:00,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.911       0.96       0.97      0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/30      4.47G      0.416     0.4345     0.9605          6        640: 100%|██████████| 47/47 [02:28<00:00,  3.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66       0.98      0.981      0.995      0.891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/30      4.47G     0.3923     0.4272     0.9481          3        640: 100%|██████████| 47/47 [03:37<00:00,  4.63s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:09<00:00,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.982      0.992      0.995      0.907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/30      4.47G     0.4092     0.4219     0.9658          6        640: 100%|██████████| 47/47 [02:20<00:00,  2.99s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.981      0.982      0.995      0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/30      4.46G     0.3997     0.3991      0.969          5        640: 100%|██████████| 47/47 [03:43<00:00,  4.75s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:11<00:00,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.965       0.99      0.995      0.881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/30      4.47G     0.3647     0.3877     0.9427          9        640: 100%|██████████| 47/47 [02:21<00:00,  3.00s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.972          1      0.995      0.892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "30 epochs completed in 2.151 hours.\n",
      "Optimizer stripped from model3\\train2\\weights\\last.pt, 5.5MB\n",
      "Optimizer stripped from model3\\train2\\weights\\best.pt, 5.5MB\n",
      "\n",
      "Validating model3\\train2\\weights\\best.pt...\n",
      "Ultralytics 8.3.70  Python-3.13.1 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1650 with Max-Q Design, 4096MiB)\n",
      "YOLO11n summary (fused): 238 layers, 2,583,127 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:00<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         35         66      0.954      0.973      0.995      0.918\n",
      "                 apple         17         30          1      0.956      0.995      0.905\n",
      "                banana         11         11          1      0.908      0.995      0.931\n",
      "                  kiwi          8         14      0.991          1      0.995      0.946\n",
      "                  pear          4          5      0.984          1      0.995      0.899\n",
      "             starfruit          5          6      0.796          1      0.995      0.909\n",
      "Speed: 1.1ms preprocess, 9.6ms inference, 0.0ms loss, 2.1ms postprocess per image\n",
      "Results saved to \u001b[1mmodel3\\train2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "#Model 3a using dataset 3 (YOLOv8)\n",
    "\"\"\"\n",
    "model = YOLO('yolov8s.pt')\n",
    "result = model.train(data=\"data.yaml\",\n",
    "                    epochs=30,\n",
    "                    save_period=1,\n",
    "                    batch=16,\n",
    "                    device=0,\n",
    "                    project='model3',\n",
    "                    plots=True,\n",
    "                    patience=20,\n",
    "                    momentum=0.937,\n",
    "                    weight_decay=0.0005,\n",
    "                    imgsz=640)\n",
    "\"\"\"\n",
    "\n",
    "#Model 3b using dataset 3 (YOLOv8)\n",
    "model = YOLO('yolo11n.pt')\n",
    "#Train the Model\n",
    "result = model.train(data=\"data.yaml\",\n",
    "                    epochs=30,\n",
    "                    save_period=1,\n",
    "                    batch=16,\n",
    "                    device=0,\n",
    "                    project='model3',\n",
    "                    plots=True,\n",
    "                    patience=20,\n",
    "                    momentum=0.937,\n",
    "                    weight_decay=0.0005,\n",
    "                    cache='True',\n",
    "                    imgsz=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a647a080",
   "metadata": {},
   "source": [
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0f08f8",
   "metadata": {},
   "source": [
    "<h2>Fine Turning: Train Model 4a using YOLOv11n<h2>\n",
    "<h3> Dataset 4 (YOLOv11) <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b843871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs4\n"
     ]
    }
   ],
   "source": [
    "target_dir = os.path.join(HOME, 'datasets', 'datasetvs4')\n",
    "os.chdir(target_dir)\n",
    "print(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0046e4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 'C:/Users/wensi/Desktop/machine_learning/object_detector/ITC110_assignment_submission/datasets/datasetvs4/train/images', 'val': 'C:/Users/wensi/Desktop/machine_learning/object_detector/ITC110_assignment_submission/datasets/datasetvs4/valid/images', 'test': 'C:/Users/wensi/Desktop/machine_learning/object_detector/ITC110_assignment_submission/datasets/datasetvs4/test/images', 'nc': 5, 'names': ['apple', 'banana', 'kiwi', 'pear', 'starfruit'], 'roboflow': {'workspace': 'itc107-assignment-1-79crc', 'project': 'nyp-itc110-project', 'version': 7, 'license': 'CC BY 4.0', 'url': 'https://universe.roboflow.com/itc107-assignment-1-79crc/nyp-itc110-project/dataset/7'}}\n",
      "Train path: C:/Users/wensi/Desktop/machine_learning/object_detector/ITC110_assignment_submission/datasets/datasetvs4/train/images\n",
      "Val path: C:/Users/wensi/Desktop/machine_learning/object_detector/ITC110_assignment_submission/datasets/datasetvs4/valid/images\n",
      "Test path: C:/Users/wensi/Desktop/machine_learning/object_detector/ITC110_assignment_submission/datasets/datasetvs4/test/images\n",
      "Number of classes: 5\n",
      "Class names: ['apple', 'banana', 'kiwi', 'pear', 'starfruit']\n",
      "Train path exists: True\n",
      "Val path exists: True\n",
      "Test path exists: True\n"
     ]
    }
   ],
   "source": [
    "#Check data loading before training\n",
    "import yaml\n",
    "\n",
    "with open(\"data.yaml\", 'r') as f:\n",
    "    data = yaml.safe_load(f)\n",
    "\n",
    "print(data)  # Print the loaded data to check for errors\n",
    "\n",
    "# Verify paths:\n",
    "print(\"Train path:\", data['train'])\n",
    "print(\"Val path:\", data['val'])\n",
    "print(\"Test path:\", data['test'])\n",
    "print(\"Number of classes:\", data['nc'])\n",
    "print(\"Class names:\", data['names'])\n",
    "\n",
    "# Check if the paths exist (optional, but helpful):\n",
    "import os\n",
    "print(\"Train path exists:\", os.path.exists(data['train']))\n",
    "print(\"Val path exists:\", os.path.exists(data['val']))\n",
    "print(\"Test path exists:\", os.path.exists(data['test']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f21c87f",
   "metadata": {},
   "source": [
    "<b>Train Model 4x</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552aa47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5.35M/5.35M [00:00<00:00, 26.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.76 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.70  Python-3.13.1 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1650 with Max-Q Design, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=data.yaml, epochs=30, time=None, patience=20, batch=16, imgsz=640, save=True, save_period=1, cache=True, device=0, workers=8, project=model4, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=model4\\train\n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    431647  ultralytics.nn.modules.head.Detect           [5, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,815 parameters, 2,590,799 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks failed . AMP training on NVIDIA GeForce GTX 1650 with Max-Q Design GPU may cause NaN losses or zero-mAP results, so AMP will be disabled during training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs4\\train\\labels... 463 images, 28 backgrounds, 0 corrupt: 100%|██████████| 463/463 [00:00<00:00, 465.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs4\\train\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs4\\valid\\labels... 55 images, 4 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<00:00, 120.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs4\\valid\\labels.cache\n",
      "Plotting labels to model4\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mmodel4\\train\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/30       4.3G      0.664      3.126      1.158         37        640: 100%|██████████| 29/29 [01:27<00:00,  3.01s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:14<00:00,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.023          1      0.428      0.348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/30      4.35G     0.7633      2.312      1.233         39        640: 100%|██████████| 29/29 [02:18<00:00,  4.77s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:14<00:00,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.815      0.354      0.667      0.561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/30      4.35G     0.7853        1.8      1.233         49        640: 100%|██████████| 29/29 [01:44<00:00,  3.61s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:15<00:00,  7.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.601      0.586       0.66      0.528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/30      4.32G     0.7006       1.47      1.154         37        640: 100%|██████████| 29/29 [03:24<00:00,  7.04s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:20<00:00, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.524      0.429      0.377      0.275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/30      4.35G     0.7107      1.307      1.139         54        640: 100%|██████████| 29/29 [01:39<00:00,  3.43s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:12<00:00,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.602      0.494      0.421      0.299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/30      4.35G     0.7058      1.297      1.134         43        640: 100%|██████████| 29/29 [01:48<00:00,  3.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:12<00:00,  6.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.655        0.8      0.733      0.598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/30      4.34G     0.6659      1.217      1.115         48        640: 100%|██████████| 29/29 [02:18<00:00,  4.77s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:11<00:00,  5.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.836       0.66      0.723      0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/30      4.36G     0.6839      1.136      1.142         46        640: 100%|██████████| 29/29 [02:46<00:00,  5.74s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:16<00:00,  8.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.776      0.755      0.766      0.616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/30      4.35G      0.639      1.022      1.092         53        640: 100%|██████████| 29/29 [01:16<00:00,  2.64s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:11<00:00,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.842      0.882      0.913      0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/30      4.36G     0.6536     0.9962      1.104         42        640: 100%|██████████| 29/29 [01:32<00:00,  3.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:11<00:00,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.907      0.985       0.98      0.853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/30      4.32G     0.6226     0.9678      1.083         40        640: 100%|██████████| 29/29 [02:24<00:00,  4.98s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:14<00:00,  7.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.908      0.976      0.973      0.828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/30      4.34G     0.6105     0.8981       1.09         46        640: 100%|██████████| 29/29 [02:22<00:00,  4.90s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:09<00:00,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.953      0.978      0.986      0.885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/30      4.35G     0.5829     0.9142      1.068         48        640: 100%|██████████| 29/29 [01:37<00:00,  3.36s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:08<00:00,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.894      0.911      0.961      0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/30      4.35G     0.5805     0.8489      1.053         36        640: 100%|██████████| 29/29 [01:39<00:00,  3.44s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:15<00:00,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.884      0.988      0.983       0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/30      4.32G     0.5772     0.8408      1.054         49        640: 100%|██████████| 29/29 [03:44<00:00,  7.75s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:16<00:00,  8.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.942      0.994      0.993      0.883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/30      4.35G     0.5523     0.7949      1.038         39        640: 100%|██████████| 29/29 [02:32<00:00,  5.26s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:16<00:00,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95       0.96      0.969      0.986      0.883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/30      4.35G     0.5409      0.758      1.035         44        640: 100%|██████████| 29/29 [01:19<00:00,  2.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:09<00:00,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.881      0.872      0.932      0.791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/30      4.35G     0.5389     0.7331      1.027         44        640: 100%|██████████| 29/29 [01:37<00:00,  3.37s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:11<00:00,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95       0.98      0.972      0.995       0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/30      4.34G     0.5508     0.7393      1.043         31        640: 100%|██████████| 29/29 [02:53<00:00,  5.99s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:16<00:00,  8.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.989      0.979      0.995      0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/30      4.34G     0.5205     0.6807      1.028         51        640: 100%|██████████| 29/29 [02:28<00:00,  5.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:10<00:00,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.894      0.964      0.986      0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/30       4.3G     0.5173     0.7811      1.061         21        640: 100%|██████████| 29/29 [01:25<00:00,  2.95s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:12<00:00,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.978      0.975      0.992       0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/30       4.3G      0.497     0.7411      1.025         18        640: 100%|██████████| 29/29 [01:47<00:00,  3.70s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:12<00:00,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.947       0.94      0.982      0.873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/30       4.3G     0.5062     0.7119      1.028         23        640: 100%|██████████| 29/29 [02:24<00:00,  4.98s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:17<00:00,  8.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.966      0.985      0.994       0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/30       4.3G     0.4637     0.6702      1.011         26        640: 100%|██████████| 29/29 [02:21<00:00,  4.89s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:11<00:00,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.954          1      0.993      0.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/30       4.3G     0.4561     0.6426     0.9898         25        640: 100%|██████████| 29/29 [01:24<00:00,  2.92s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:12<00:00,  6.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.987      0.983      0.993      0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/30       4.3G     0.4577     0.6075       1.01         28        640: 100%|██████████| 29/29 [01:28<00:00,  3.07s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:13<00:00,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.958      0.993      0.991      0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/30       4.3G     0.4475     0.6053     0.9895         26        640: 100%|██████████| 29/29 [03:43<00:00,  7.70s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:18<00:00,  9.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.963      0.971      0.993      0.907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/30       4.3G     0.4181     0.5738     0.9775         19        640: 100%|██████████| 29/29 [02:38<00:00,  5.45s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:19<00:00,  9.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.975       0.99      0.995      0.915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/30       4.3G     0.4307      0.576     0.9935         18        640: 100%|██████████| 29/29 [01:07<00:00,  2.33s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:11<00:00,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95       0.97      0.997      0.995      0.913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/30       4.3G     0.4402     0.5863     0.9955         23        640: 100%|██████████| 29/29 [01:49<00:00,  3.77s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:12<00:00,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.968      0.996      0.995      0.911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "30 epochs completed in 1.203 hours.\n",
      "Optimizer stripped from model4\\train\\weights\\last.pt, 5.5MB\n",
      "Optimizer stripped from model4\\train\\weights\\best.pt, 5.5MB\n",
      "\n",
      "Validating model4\\train\\weights\\best.pt...\n",
      "Ultralytics 8.3.70  Python-3.13.1 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1650 with Max-Q Design, 4096MiB)\n",
      "YOLO11n summary (fused): 238 layers, 2,583,127 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.989      0.974      0.994      0.913\n",
      "                 apple         17         30      0.992          1      0.995      0.924\n",
      "                banana         11         11      0.986          1      0.995      0.883\n",
      "                  kiwi          8         14      0.988          1      0.995      0.946\n",
      "                  pear          9         13          1      0.868       0.99      0.867\n",
      "             starfruit         18         27      0.981          1      0.995      0.945\n",
      "Speed: 1.0ms preprocess, 12.1ms inference, 0.0ms loss, 4.0ms postprocess per image\n",
      "Results saved to \u001b[1mmodel4\\train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "#Model 4a using dataset 4 (YOLOv11)\n",
    "model = YOLO('yolo11n.pt')\n",
    "\n",
    "#Train the Model\n",
    "result = model.train(data=\"data.yaml\",\n",
    "                    epochs=30,\n",
    "                    save_period=1,\n",
    "                    batch=16,\n",
    "                    device=0,\n",
    "                    project='model4',\n",
    "                    plots=True,\n",
    "                    patience=20,\n",
    "                    momentum=0.937,\n",
    "                    weight_decay=0.0005,\n",
    "                    cache='True',\n",
    "                    imgsz=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78b5a90",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4574c359",
   "metadata": {},
   "source": [
    "<h1> Validation and Testing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66341d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.70  Python-3.13.1 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce GTX 1650 with Max-Q Design, 4096MiB)\n",
      "YOLO11n summary (fused): 238 layers, 2,583,127 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs4_yolov11\\valid\\labels... 55 images, 4 backgrounds, 0 corrupt: 100%|██████████| 55/55 [00:00<00:00, 333.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs4_yolov11\\valid\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:13<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         55         95      0.989      0.974      0.994      0.913\n",
      "                 apple         17         30      0.992          1      0.995      0.924\n",
      "                banana         11         11      0.986          1      0.995      0.883\n",
      "                  kiwi          8         14      0.988          1      0.995      0.946\n",
      "                  pear          9         13          1      0.868       0.99      0.867\n",
      "             starfruit         18         27      0.981          1      0.995      0.945\n",
      "Speed: 3.3ms preprocess, 12.1ms inference, 0.0ms loss, 13.4ms postprocess per image\n",
      "Results saved to \u001b[1mc:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\runs\\detect\\val17\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "target_dir = os.path.join(HOME, 'datasets', 'datasetvs4_yolov11')\n",
    "os.chdir(target_dir)\n",
    "\n",
    "#Load the Model\n",
    "mdoel_to_val = YOLO('model4/train_epoch30_yolov11n/weights/best.pt')\n",
    "#Validate the Model\n",
    "metrics = mdoel_to_val.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666ad3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_15_47_50_Pro.jpg: 384x640 1 apple, 54.1ms\n",
      "image 2/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_15_48_01_Pro.jpg: 384x640 2 apples, 15.1ms\n",
      "image 3/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_15_51_04_Pro.jpg: 384x640 2 apples, 1 pear, 14.8ms\n",
      "image 4/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_15_52_20_Pro.jpg: 384x640 4 apples, 12.7ms\n",
      "image 5/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_15_52_30_Pro.jpg: 384x640 4 apples, 1 pear, 13.8ms\n",
      "image 6/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_15_56_34_Pro.jpg: 384x640 1 banana, 13.2ms\n",
      "image 7/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_15_57_01_Pro.jpg: 384x640 1 banana, 19.7ms\n",
      "image 8/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_15_57_41_Pro.jpg: 384x640 1 banana, 16.8ms\n",
      "image 9/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_15_58_36_Pro.jpg: 384x640 2 bananas, 15.1ms\n",
      "image 10/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_15_59_34_Pro.jpg: 384x640 2 bananas, 25.8ms\n",
      "image 11/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_16_03_07_Pro.jpg: 384x640 1 pear, 22.2ms\n",
      "image 12/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_16_03_37_Pro.jpg: 384x640 2 pears, 12.4ms\n",
      "image 13/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_16_04_30_Pro.jpg: 384x640 3 pears, 12.3ms\n",
      "image 14/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_16_05_40_Pro.jpg: 384x640 4 pears, 13.6ms\n",
      "image 15/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_16_07_22_Pro.jpg: 384x640 5 pears, 15.2ms\n",
      "image 16/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_16_11_11_Pro.jpg: 384x640 1 kiwi, 16.8ms\n",
      "image 17/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_16_12_08_Pro.jpg: 384x640 2 kiwis, 23.0ms\n",
      "image 18/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_16_13_04_Pro.jpg: 384x640 3 kiwis, 24.0ms\n",
      "image 19/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_16_13_27_Pro.jpg: 384x640 3 kiwis, 14.6ms\n",
      "image 20/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_16_13_53_Pro.jpg: 384x640 3 kiwis, 13.0ms\n",
      "image 21/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_16_18_23_Pro.jpg: 384x640 1 starfruit, 12.3ms\n",
      "image 22/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_16_18_54_Pro.jpg: 384x640 2 starfruits, 15.3ms\n",
      "image 23/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_16_19_59_Pro.jpg: 384x640 3 starfruits, 13.4ms\n",
      "image 24/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_16_21_50_Pro.jpg: 384x640 4 starfruits, 13.8ms\n",
      "image 25/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_16_22_20_Pro.jpg: 384x640 4 starfruits, 13.8ms\n",
      "image 26/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_16_25_17_Pro.jpg: 384x640 1 apple, 1 banana, 1 kiwi, 1 pear, 1 starfruit, 13.3ms\n",
      "image 27/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_16_27_08_Pro.jpg: 384x640 1 banana, 1 kiwi, 1 pear, 1 starfruit, 15.7ms\n",
      "image 28/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_16_27_27_Pro.jpg: 384x640 1 apple, 1 kiwi, 1 starfruit, 17.1ms\n",
      "image 29/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_16_31_25_Pro.jpg: 384x640 1 apple, 1 banana, 1 kiwi, 1 pear, 1 starfruit, 15.9ms\n",
      "image 30/30 c:\\Users\\wensi\\Desktop\\machine_learning\\object_detector\\ITC110_assignment_submission\\datasets\\datasetvs1_yolvo11\\..\\..\\data_for_testing_set1\\WIN_20250202_16_32_20_Pro.jpg: 384x640 1 apple, 1 kiwi, 1 pear, 1 starfruit, 13.0ms\n",
      "Speed: 2.4ms preprocess, 17.1ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[143, 143, 137],\n",
      "        [144, 144, 138],\n",
      "        [142, 142, 136],\n",
      "        ...,\n",
      "        [117, 115, 114],\n",
      "        [122, 120, 119],\n",
      "        [130, 128, 127]],\n",
      "\n",
      "       [[141, 141, 135],\n",
      "        [141, 141, 135],\n",
      "        [139, 139, 133],\n",
      "        ...,\n",
      "        [124, 122, 121],\n",
      "        [125, 123, 122],\n",
      "        [124, 122, 121]],\n",
      "\n",
      "       [[144, 144, 138],\n",
      "        [143, 143, 137],\n",
      "        [139, 139, 133],\n",
      "        ...,\n",
      "        [132, 131, 127],\n",
      "        [129, 128, 124],\n",
      "        [120, 119, 115]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[135, 135, 129],\n",
      "        [136, 136, 130],\n",
      "        [136, 136, 130],\n",
      "        ...,\n",
      "        [133, 133, 127],\n",
      "        [135, 135, 129],\n",
      "        [133, 133, 127]],\n",
      "\n",
      "       [[132, 131, 127],\n",
      "        [133, 132, 128],\n",
      "        [132, 131, 127],\n",
      "        ...,\n",
      "        [133, 132, 128],\n",
      "        [134, 133, 129],\n",
      "        [132, 131, 127]],\n",
      "\n",
      "       [[130, 129, 125],\n",
      "        [132, 131, 127],\n",
      "        [137, 136, 132],\n",
      "        ...,\n",
      "        [155, 154, 150],\n",
      "        [155, 154, 150],\n",
      "        [154, 153, 149]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_15_47_50_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.8450489044189453, 'inference': 54.146766662597656, 'postprocess': 8.132219314575195}\n",
      "Detected Classes: {0}\n",
      "Number of detected bounding boxes: 1\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[144, 144, 138],\n",
      "        [142, 142, 136],\n",
      "        [139, 139, 133],\n",
      "        ...,\n",
      "        [121, 120, 116],\n",
      "        [124, 123, 119],\n",
      "        [122, 121, 117]],\n",
      "\n",
      "       [[144, 144, 138],\n",
      "        [142, 142, 136],\n",
      "        [140, 140, 134],\n",
      "        ...,\n",
      "        [121, 120, 116],\n",
      "        [124, 123, 119],\n",
      "        [118, 117, 113]],\n",
      "\n",
      "       [[144, 144, 138],\n",
      "        [143, 143, 137],\n",
      "        [142, 142, 136],\n",
      "        ...,\n",
      "        [124, 123, 119],\n",
      "        [125, 124, 120],\n",
      "        [118, 117, 113]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[138, 137, 133],\n",
      "        [140, 139, 135],\n",
      "        [140, 139, 135],\n",
      "        ...,\n",
      "        [130, 130, 124],\n",
      "        [134, 134, 128],\n",
      "        [137, 137, 131]],\n",
      "\n",
      "       [[132, 131, 127],\n",
      "        [139, 138, 134],\n",
      "        [142, 141, 137],\n",
      "        ...,\n",
      "        [140, 139, 135],\n",
      "        [138, 137, 133],\n",
      "        [136, 135, 131]],\n",
      "\n",
      "       [[142, 141, 137],\n",
      "        [153, 152, 148],\n",
      "        [160, 159, 155],\n",
      "        ...,\n",
      "        [153, 152, 148],\n",
      "        [139, 138, 134],\n",
      "        [128, 127, 123]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_15_48_01_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 3.0357837677001953, 'inference': 15.148401260375977, 'postprocess': 2.414703369140625}\n",
      "Detected Classes: {0}\n",
      "Number of detected bounding boxes: 2\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[136, 136, 130],\n",
      "        [138, 138, 132],\n",
      "        [139, 139, 133],\n",
      "        ...,\n",
      "        [122, 121, 117],\n",
      "        [127, 126, 122],\n",
      "        [120, 119, 115]],\n",
      "\n",
      "       [[138, 138, 132],\n",
      "        [139, 139, 133],\n",
      "        [139, 139, 133],\n",
      "        ...,\n",
      "        [124, 123, 119],\n",
      "        [128, 127, 123],\n",
      "        [120, 119, 115]],\n",
      "\n",
      "       [[137, 137, 131],\n",
      "        [140, 140, 134],\n",
      "        [141, 141, 135],\n",
      "        ...,\n",
      "        [128, 127, 123],\n",
      "        [117, 116, 112],\n",
      "        [126, 125, 121]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[136, 135, 131],\n",
      "        [135, 134, 130],\n",
      "        [134, 133, 129],\n",
      "        ...,\n",
      "        [140, 139, 135],\n",
      "        [135, 134, 130],\n",
      "        [134, 133, 129]],\n",
      "\n",
      "       [[144, 143, 139],\n",
      "        [144, 143, 139],\n",
      "        [143, 142, 138],\n",
      "        ...,\n",
      "        [132, 131, 127],\n",
      "        [137, 136, 132],\n",
      "        [133, 132, 128]],\n",
      "\n",
      "       [[129, 128, 124],\n",
      "        [138, 137, 133],\n",
      "        [150, 149, 145],\n",
      "        ...,\n",
      "        [163, 162, 158],\n",
      "        [167, 166, 162],\n",
      "        [154, 153, 149]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_15_51_04_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.1338462829589844, 'inference': 14.79482650756836, 'postprocess': 2.4330615997314453}\n",
      "Detected Classes: {0, 3}\n",
      "Number of detected bounding boxes: 3\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[143, 143, 137],\n",
      "        [140, 140, 134],\n",
      "        [138, 138, 132],\n",
      "        ...,\n",
      "        [126, 125, 121],\n",
      "        [119, 118, 114],\n",
      "        [121, 120, 116]],\n",
      "\n",
      "       [[144, 144, 138],\n",
      "        [141, 141, 135],\n",
      "        [140, 140, 134],\n",
      "        ...,\n",
      "        [129, 128, 124],\n",
      "        [124, 123, 119],\n",
      "        [123, 122, 118]],\n",
      "\n",
      "       [[142, 142, 136],\n",
      "        [142, 142, 136],\n",
      "        [142, 142, 136],\n",
      "        ...,\n",
      "        [123, 122, 118],\n",
      "        [126, 125, 121],\n",
      "        [130, 129, 125]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[133, 132, 128],\n",
      "        [139, 138, 134],\n",
      "        [139, 138, 134],\n",
      "        ...,\n",
      "        [136, 135, 131],\n",
      "        [136, 135, 131],\n",
      "        [133, 132, 128]],\n",
      "\n",
      "       [[143, 142, 138],\n",
      "        [147, 146, 142],\n",
      "        [147, 146, 142],\n",
      "        ...,\n",
      "        [142, 141, 137],\n",
      "        [145, 144, 140],\n",
      "        [136, 135, 131]],\n",
      "\n",
      "       [[134, 133, 129],\n",
      "        [135, 134, 130],\n",
      "        [139, 138, 134],\n",
      "        ...,\n",
      "        [167, 166, 162],\n",
      "        [167, 166, 162],\n",
      "        [156, 155, 151]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_15_52_20_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.031564712524414, 'inference': 12.726068496704102, 'postprocess': 2.3889541625976562}\n",
      "Detected Classes: {0}\n",
      "Number of detected bounding boxes: 4\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[137, 139, 133],\n",
      "        [140, 142, 136],\n",
      "        [138, 140, 134],\n",
      "        ...,\n",
      "        [121, 121, 115],\n",
      "        [122, 122, 116],\n",
      "        [126, 126, 120]],\n",
      "\n",
      "       [[135, 137, 131],\n",
      "        [136, 138, 132],\n",
      "        [134, 136, 130],\n",
      "        ...,\n",
      "        [123, 123, 117],\n",
      "        [124, 124, 118],\n",
      "        [128, 128, 122]],\n",
      "\n",
      "       [[142, 144, 138],\n",
      "        [140, 142, 136],\n",
      "        [139, 141, 135],\n",
      "        ...,\n",
      "        [123, 123, 117],\n",
      "        [126, 126, 120],\n",
      "        [131, 131, 125]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[144, 146, 140],\n",
      "        [145, 147, 141],\n",
      "        [148, 150, 144],\n",
      "        ...,\n",
      "        [141, 140, 136],\n",
      "        [141, 140, 136],\n",
      "        [132, 131, 127]],\n",
      "\n",
      "       [[139, 140, 136],\n",
      "        [141, 142, 138],\n",
      "        [144, 145, 141],\n",
      "        ...,\n",
      "        [137, 136, 132],\n",
      "        [150, 149, 145],\n",
      "        [144, 143, 139]],\n",
      "\n",
      "       [[145, 146, 142],\n",
      "        [153, 154, 150],\n",
      "        [164, 165, 161],\n",
      "        ...,\n",
      "        [164, 163, 159],\n",
      "        [164, 163, 159],\n",
      "        [141, 140, 136]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_15_52_30_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.033710479736328, 'inference': 13.826847076416016, 'postprocess': 2.0172595977783203}\n",
      "Detected Classes: {0, 3}\n",
      "Number of detected bounding boxes: 5\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[139, 141, 135],\n",
      "        [139, 141, 135],\n",
      "        [139, 141, 135],\n",
      "        ...,\n",
      "        [125, 126, 117],\n",
      "        [126, 127, 118],\n",
      "        [121, 122, 113]],\n",
      "\n",
      "       [[138, 140, 134],\n",
      "        [138, 140, 134],\n",
      "        [138, 140, 134],\n",
      "        ...,\n",
      "        [122, 123, 114],\n",
      "        [123, 124, 115],\n",
      "        [120, 121, 112]],\n",
      "\n",
      "       [[137, 139, 133],\n",
      "        [137, 139, 133],\n",
      "        [137, 139, 133],\n",
      "        ...,\n",
      "        [119, 119, 113],\n",
      "        [123, 123, 117],\n",
      "        [123, 123, 117]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[138, 140, 134],\n",
      "        [140, 142, 136],\n",
      "        [136, 138, 132],\n",
      "        ...,\n",
      "        [127, 128, 126],\n",
      "        [128, 129, 127],\n",
      "        [137, 138, 136]],\n",
      "\n",
      "       [[129, 130, 126],\n",
      "        [138, 139, 135],\n",
      "        [140, 141, 137],\n",
      "        ...,\n",
      "        [140, 141, 139],\n",
      "        [135, 136, 134],\n",
      "        [133, 134, 132]],\n",
      "\n",
      "       [[133, 134, 130],\n",
      "        [154, 155, 151],\n",
      "        [167, 168, 164],\n",
      "        ...,\n",
      "        [153, 154, 152],\n",
      "        [160, 161, 159],\n",
      "        [161, 162, 160]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_15_56_34_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 1.9748210906982422, 'inference': 13.205766677856445, 'postprocess': 2.0017623901367188}\n",
      "Detected Classes: {1}\n",
      "Number of detected bounding boxes: 1\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[134, 136, 130],\n",
      "        [134, 136, 130],\n",
      "        [135, 137, 131],\n",
      "        ...,\n",
      "        [118, 116, 116],\n",
      "        [123, 121, 121],\n",
      "        [120, 118, 118]],\n",
      "\n",
      "       [[138, 140, 134],\n",
      "        [134, 136, 130],\n",
      "        [132, 134, 128],\n",
      "        ...,\n",
      "        [121, 119, 119],\n",
      "        [128, 126, 126],\n",
      "        [124, 122, 122]],\n",
      "\n",
      "       [[137, 139, 133],\n",
      "        [135, 137, 131],\n",
      "        [134, 136, 130],\n",
      "        ...,\n",
      "        [120, 118, 117],\n",
      "        [125, 123, 123],\n",
      "        [118, 116, 116]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[142, 144, 138],\n",
      "        [139, 141, 135],\n",
      "        [138, 140, 134],\n",
      "        ...,\n",
      "        [137, 135, 134],\n",
      "        [134, 132, 131],\n",
      "        [136, 134, 133]],\n",
      "\n",
      "       [[131, 132, 128],\n",
      "        [134, 135, 131],\n",
      "        [136, 137, 133],\n",
      "        ...,\n",
      "        [141, 139, 138],\n",
      "        [139, 137, 136],\n",
      "        [133, 131, 130]],\n",
      "\n",
      "       [[135, 136, 132],\n",
      "        [147, 148, 144],\n",
      "        [161, 162, 158],\n",
      "        ...,\n",
      "        [160, 158, 157],\n",
      "        [155, 153, 152],\n",
      "        [131, 129, 128]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_15_57_01_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.3040771484375, 'inference': 19.71292495727539, 'postprocess': 2.5370121002197266}\n",
      "Detected Classes: {1}\n",
      "Number of detected bounding boxes: 1\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[139, 139, 133],\n",
      "        [138, 138, 132],\n",
      "        [138, 138, 132],\n",
      "        ...,\n",
      "        [121, 120, 116],\n",
      "        [127, 126, 122],\n",
      "        [124, 123, 119]],\n",
      "\n",
      "       [[138, 138, 132],\n",
      "        [138, 138, 132],\n",
      "        [139, 139, 133],\n",
      "        ...,\n",
      "        [127, 126, 122],\n",
      "        [125, 124, 120],\n",
      "        [118, 117, 113]],\n",
      "\n",
      "       [[137, 137, 131],\n",
      "        [137, 137, 131],\n",
      "        [138, 138, 132],\n",
      "        ...,\n",
      "        [126, 124, 123],\n",
      "        [120, 118, 117],\n",
      "        [115, 113, 112]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[138, 138, 132],\n",
      "        [142, 142, 136],\n",
      "        [144, 144, 138],\n",
      "        ...,\n",
      "        [133, 132, 128],\n",
      "        [136, 135, 131],\n",
      "        [132, 131, 127]],\n",
      "\n",
      "       [[140, 139, 135],\n",
      "        [145, 144, 140],\n",
      "        [144, 143, 139],\n",
      "        ...,\n",
      "        [135, 133, 132],\n",
      "        [137, 135, 134],\n",
      "        [132, 130, 129]],\n",
      "\n",
      "       [[135, 134, 130],\n",
      "        [135, 134, 130],\n",
      "        [133, 132, 128],\n",
      "        ...,\n",
      "        [159, 157, 156],\n",
      "        [162, 160, 159],\n",
      "        [157, 155, 154]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_15_57_41_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.1944046020507812, 'inference': 16.76630973815918, 'postprocess': 1.8818378448486328}\n",
      "Detected Classes: {1}\n",
      "Number of detected bounding boxes: 1\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[139, 138, 134],\n",
      "        [143, 142, 138],\n",
      "        [138, 137, 133],\n",
      "        ...,\n",
      "        [121, 118, 114],\n",
      "        [125, 122, 118],\n",
      "        [126, 123, 119]],\n",
      "\n",
      "       [[142, 141, 137],\n",
      "        [136, 135, 131],\n",
      "        [129, 128, 124],\n",
      "        ...,\n",
      "        [125, 122, 118],\n",
      "        [125, 122, 118],\n",
      "        [126, 123, 119]],\n",
      "\n",
      "       [[139, 139, 133],\n",
      "        [135, 135, 129],\n",
      "        [137, 137, 131],\n",
      "        ...,\n",
      "        [124, 121, 117],\n",
      "        [123, 120, 116],\n",
      "        [126, 123, 119]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[129, 129, 123],\n",
      "        [130, 130, 124],\n",
      "        [136, 136, 130],\n",
      "        ...,\n",
      "        [130, 130, 124],\n",
      "        [130, 130, 124],\n",
      "        [136, 136, 130]],\n",
      "\n",
      "       [[130, 132, 126],\n",
      "        [124, 126, 120],\n",
      "        [130, 132, 126],\n",
      "        ...,\n",
      "        [127, 126, 122],\n",
      "        [128, 127, 123],\n",
      "        [137, 136, 132]],\n",
      "\n",
      "       [[124, 126, 120],\n",
      "        [126, 128, 122],\n",
      "        [153, 155, 149],\n",
      "        ...,\n",
      "        [159, 158, 154],\n",
      "        [158, 157, 153],\n",
      "        [163, 162, 158]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_15_58_36_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.045154571533203, 'inference': 15.113115310668945, 'postprocess': 2.5289058685302734}\n",
      "Detected Classes: {1}\n",
      "Number of detected bounding boxes: 2\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[142, 139, 134],\n",
      "        [143, 140, 135],\n",
      "        [143, 140, 135],\n",
      "        ...,\n",
      "        [128, 127, 123],\n",
      "        [124, 123, 119],\n",
      "        [129, 128, 124]],\n",
      "\n",
      "       [[143, 140, 135],\n",
      "        [143, 140, 135],\n",
      "        [143, 140, 135],\n",
      "        ...,\n",
      "        [128, 127, 123],\n",
      "        [125, 124, 120],\n",
      "        [128, 127, 123]],\n",
      "\n",
      "       [[143, 140, 135],\n",
      "        [143, 140, 135],\n",
      "        [144, 141, 136],\n",
      "        ...,\n",
      "        [128, 127, 123],\n",
      "        [126, 125, 121],\n",
      "        [128, 127, 123]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[150, 150, 144],\n",
      "        [145, 145, 139],\n",
      "        [145, 145, 139],\n",
      "        ...,\n",
      "        [137, 136, 132],\n",
      "        [138, 137, 133],\n",
      "        [135, 134, 130]],\n",
      "\n",
      "       [[144, 143, 139],\n",
      "        [141, 140, 136],\n",
      "        [146, 145, 141],\n",
      "        ...,\n",
      "        [133, 134, 132],\n",
      "        [134, 135, 133],\n",
      "        [133, 134, 132]],\n",
      "\n",
      "       [[142, 141, 137],\n",
      "        [145, 144, 140],\n",
      "        [161, 160, 156],\n",
      "        ...,\n",
      "        [156, 157, 155],\n",
      "        [160, 161, 159],\n",
      "        [160, 161, 159]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_15_59_34_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.296924591064453, 'inference': 25.84242820739746, 'postprocess': 1.9993782043457031}\n",
      "Detected Classes: {1}\n",
      "Number of detected bounding boxes: 2\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[132, 132, 132],\n",
      "        [133, 133, 133],\n",
      "        [136, 136, 136],\n",
      "        ...,\n",
      "        [130, 127, 122],\n",
      "        [127, 124, 119],\n",
      "        [124, 121, 116]],\n",
      "\n",
      "       [[136, 136, 136],\n",
      "        [133, 133, 133],\n",
      "        [130, 130, 130],\n",
      "        ...,\n",
      "        [119, 116, 111],\n",
      "        [124, 121, 116],\n",
      "        [132, 129, 124]],\n",
      "\n",
      "       [[135, 135, 135],\n",
      "        [139, 139, 139],\n",
      "        [144, 145, 143],\n",
      "        ...,\n",
      "        [133, 132, 128],\n",
      "        [123, 122, 118],\n",
      "        [118, 117, 113]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[144, 142, 141],\n",
      "        [144, 142, 141],\n",
      "        [138, 136, 135],\n",
      "        ...,\n",
      "        [122, 123, 121],\n",
      "        [128, 129, 127],\n",
      "        [126, 127, 125]],\n",
      "\n",
      "       [[138, 136, 135],\n",
      "        [142, 140, 139],\n",
      "        [140, 138, 137],\n",
      "        ...,\n",
      "        [124, 125, 123],\n",
      "        [133, 134, 132],\n",
      "        [127, 128, 126]],\n",
      "\n",
      "       [[142, 140, 139],\n",
      "        [154, 152, 151],\n",
      "        [159, 157, 156],\n",
      "        ...,\n",
      "        [152, 153, 151],\n",
      "        [160, 161, 159],\n",
      "        [151, 152, 150]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_16_03_07_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.2530555725097656, 'inference': 22.225379943847656, 'postprocess': 1.8053054809570312}\n",
      "Detected Classes: {3}\n",
      "Number of detected bounding boxes: 1\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[136, 135, 131],\n",
      "        [138, 137, 133],\n",
      "        [138, 137, 133],\n",
      "        ...,\n",
      "        [128, 129, 127],\n",
      "        [123, 124, 122],\n",
      "        [121, 122, 120]],\n",
      "\n",
      "       [[134, 133, 129],\n",
      "        [138, 137, 133],\n",
      "        [140, 139, 135],\n",
      "        ...,\n",
      "        [126, 127, 125],\n",
      "        [122, 123, 121],\n",
      "        [120, 121, 119]],\n",
      "\n",
      "       [[130, 129, 125],\n",
      "        [134, 133, 129],\n",
      "        [137, 136, 132],\n",
      "        ...,\n",
      "        [125, 126, 124],\n",
      "        [122, 123, 121],\n",
      "        [121, 122, 120]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[141, 142, 140],\n",
      "        [138, 139, 137],\n",
      "        [136, 137, 135],\n",
      "        ...,\n",
      "        [128, 129, 127],\n",
      "        [131, 132, 130],\n",
      "        [126, 127, 125]],\n",
      "\n",
      "       [[137, 138, 136],\n",
      "        [136, 137, 135],\n",
      "        [137, 138, 136],\n",
      "        ...,\n",
      "        [132, 133, 131],\n",
      "        [131, 132, 130],\n",
      "        [119, 120, 118]],\n",
      "\n",
      "       [[134, 135, 133],\n",
      "        [141, 142, 140],\n",
      "        [150, 151, 149],\n",
      "        ...,\n",
      "        [153, 154, 152],\n",
      "        [158, 159, 157],\n",
      "        [151, 152, 150]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_16_03_37_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.0325183868408203, 'inference': 12.408256530761719, 'postprocess': 2.4237632751464844}\n",
      "Detected Classes: {3}\n",
      "Number of detected bounding boxes: 2\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[139, 138, 134],\n",
      "        [141, 140, 136],\n",
      "        [143, 142, 138],\n",
      "        ...,\n",
      "        [122, 120, 119],\n",
      "        [125, 123, 122],\n",
      "        [123, 121, 120]],\n",
      "\n",
      "       [[139, 138, 134],\n",
      "        [138, 137, 133],\n",
      "        [138, 137, 133],\n",
      "        ...,\n",
      "        [125, 123, 122],\n",
      "        [126, 124, 123],\n",
      "        [119, 117, 116]],\n",
      "\n",
      "       [[136, 135, 131],\n",
      "        [136, 135, 131],\n",
      "        [139, 138, 134],\n",
      "        ...,\n",
      "        [130, 128, 127],\n",
      "        [128, 126, 125],\n",
      "        [124, 122, 121]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[135, 136, 134],\n",
      "        [125, 126, 124],\n",
      "        [125, 126, 124],\n",
      "        ...,\n",
      "        [128, 129, 127],\n",
      "        [133, 134, 132],\n",
      "        [138, 139, 137]],\n",
      "\n",
      "       [[140, 141, 139],\n",
      "        [133, 134, 132],\n",
      "        [133, 134, 132],\n",
      "        ...,\n",
      "        [128, 129, 127],\n",
      "        [126, 127, 125],\n",
      "        [127, 128, 126]],\n",
      "\n",
      "       [[132, 133, 131],\n",
      "        [146, 147, 145],\n",
      "        [157, 158, 156],\n",
      "        ...,\n",
      "        [156, 157, 155],\n",
      "        [157, 158, 156],\n",
      "        [161, 162, 160]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_16_04_30_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.245664596557617, 'inference': 12.285709381103516, 'postprocess': 1.8324851989746094}\n",
      "Detected Classes: {3}\n",
      "Number of detected bounding boxes: 3\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[138, 138, 132],\n",
      "        [135, 135, 129],\n",
      "        [139, 139, 133],\n",
      "        ...,\n",
      "        [112, 117, 116],\n",
      "        [122, 127, 126],\n",
      "        [120, 125, 124]],\n",
      "\n",
      "       [[139, 139, 133],\n",
      "        [139, 139, 133],\n",
      "        [139, 139, 133],\n",
      "        ...,\n",
      "        [117, 122, 121],\n",
      "        [116, 121, 120],\n",
      "        [118, 123, 122]],\n",
      "\n",
      "       [[137, 137, 131],\n",
      "        [138, 138, 132],\n",
      "        [131, 131, 125],\n",
      "        ...,\n",
      "        [124, 129, 127],\n",
      "        [115, 120, 118],\n",
      "        [123, 128, 126]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[140, 140, 134],\n",
      "        [141, 141, 135],\n",
      "        [140, 140, 134],\n",
      "        ...,\n",
      "        [126, 127, 125],\n",
      "        [128, 129, 127],\n",
      "        [119, 120, 118]],\n",
      "\n",
      "       [[144, 143, 139],\n",
      "        [148, 147, 143],\n",
      "        [148, 147, 143],\n",
      "        ...,\n",
      "        [129, 130, 128],\n",
      "        [128, 129, 127],\n",
      "        [120, 121, 119]],\n",
      "\n",
      "       [[148, 147, 143],\n",
      "        [158, 157, 153],\n",
      "        [165, 164, 160],\n",
      "        ...,\n",
      "        [151, 152, 150],\n",
      "        [151, 152, 150],\n",
      "        [154, 155, 153]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_16_05_40_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 1.9147396087646484, 'inference': 13.644933700561523, 'postprocess': 1.7440319061279297}\n",
      "Detected Classes: {3}\n",
      "Number of detected bounding boxes: 4\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[141, 138, 133],\n",
      "        [140, 137, 132],\n",
      "        [139, 136, 131],\n",
      "        ...,\n",
      "        [127, 126, 122],\n",
      "        [118, 117, 113],\n",
      "        [125, 124, 120]],\n",
      "\n",
      "       [[138, 135, 130],\n",
      "        [138, 135, 130],\n",
      "        [138, 135, 130],\n",
      "        ...,\n",
      "        [113, 112, 108],\n",
      "        [120, 119, 115],\n",
      "        [124, 123, 119]],\n",
      "\n",
      "       [[139, 136, 131],\n",
      "        [140, 137, 132],\n",
      "        [139, 136, 131],\n",
      "        ...,\n",
      "        [113, 112, 108],\n",
      "        [131, 130, 126],\n",
      "        [122, 121, 117]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[140, 140, 134],\n",
      "        [142, 142, 136],\n",
      "        [140, 140, 134],\n",
      "        ...,\n",
      "        [123, 124, 122],\n",
      "        [121, 122, 120],\n",
      "        [121, 122, 120]],\n",
      "\n",
      "       [[143, 142, 138],\n",
      "        [149, 148, 144],\n",
      "        [148, 147, 143],\n",
      "        ...,\n",
      "        [125, 126, 124],\n",
      "        [122, 123, 121],\n",
      "        [119, 120, 118]],\n",
      "\n",
      "       [[139, 138, 134],\n",
      "        [156, 155, 151],\n",
      "        [168, 167, 163],\n",
      "        ...,\n",
      "        [154, 155, 153],\n",
      "        [155, 156, 154],\n",
      "        [149, 150, 148]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_16_07_22_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.2788047790527344, 'inference': 15.215158462524414, 'postprocess': 1.7693042755126953}\n",
      "Detected Classes: {3}\n",
      "Number of detected bounding boxes: 5\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[139, 139, 133],\n",
      "        [140, 140, 134],\n",
      "        [140, 140, 134],\n",
      "        ...,\n",
      "        [121, 119, 118],\n",
      "        [120, 118, 117],\n",
      "        [123, 121, 120]],\n",
      "\n",
      "       [[138, 138, 132],\n",
      "        [138, 138, 132],\n",
      "        [138, 138, 132],\n",
      "        ...,\n",
      "        [121, 119, 118],\n",
      "        [132, 130, 129],\n",
      "        [119, 117, 116]],\n",
      "\n",
      "       [[137, 137, 131],\n",
      "        [138, 138, 132],\n",
      "        [139, 139, 133],\n",
      "        ...,\n",
      "        [121, 119, 118],\n",
      "        [131, 129, 128],\n",
      "        [117, 115, 114]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[143, 143, 137],\n",
      "        [142, 142, 136],\n",
      "        [140, 140, 134],\n",
      "        ...,\n",
      "        [125, 126, 124],\n",
      "        [130, 131, 129],\n",
      "        [131, 132, 130]],\n",
      "\n",
      "       [[138, 137, 133],\n",
      "        [141, 140, 136],\n",
      "        [140, 139, 135],\n",
      "        ...,\n",
      "        [129, 130, 128],\n",
      "        [134, 135, 133],\n",
      "        [116, 117, 115]],\n",
      "\n",
      "       [[139, 138, 134],\n",
      "        [151, 150, 146],\n",
      "        [161, 160, 156],\n",
      "        ...,\n",
      "        [152, 153, 151],\n",
      "        [153, 154, 152],\n",
      "        [157, 158, 156]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_16_11_11_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.232074737548828, 'inference': 16.782283782958984, 'postprocess': 3.943920135498047}\n",
      "Detected Classes: {2}\n",
      "Number of detected bounding boxes: 1\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[141, 141, 135],\n",
      "        [141, 141, 135],\n",
      "        [138, 138, 132],\n",
      "        ...,\n",
      "        [126, 127, 125],\n",
      "        [122, 123, 121],\n",
      "        [126, 127, 125]],\n",
      "\n",
      "       [[141, 141, 135],\n",
      "        [142, 142, 136],\n",
      "        [139, 139, 133],\n",
      "        ...,\n",
      "        [122, 123, 121],\n",
      "        [117, 118, 116],\n",
      "        [120, 121, 119]],\n",
      "\n",
      "       [[141, 141, 135],\n",
      "        [143, 143, 137],\n",
      "        [140, 140, 134],\n",
      "        ...,\n",
      "        [127, 128, 126],\n",
      "        [125, 126, 124],\n",
      "        [128, 129, 127]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[136, 136, 130],\n",
      "        [141, 141, 135],\n",
      "        [141, 141, 135],\n",
      "        ...,\n",
      "        [134, 135, 133],\n",
      "        [129, 130, 128],\n",
      "        [131, 132, 130]],\n",
      "\n",
      "       [[141, 140, 136],\n",
      "        [140, 139, 135],\n",
      "        [136, 135, 131],\n",
      "        ...,\n",
      "        [132, 133, 131],\n",
      "        [133, 134, 132],\n",
      "        [126, 127, 125]],\n",
      "\n",
      "       [[137, 136, 132],\n",
      "        [145, 144, 140],\n",
      "        [154, 153, 149],\n",
      "        ...,\n",
      "        [156, 157, 155],\n",
      "        [151, 152, 150],\n",
      "        [124, 125, 123]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_16_12_08_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.518177032470703, 'inference': 23.029565811157227, 'postprocess': 3.875255584716797}\n",
      "Detected Classes: {2}\n",
      "Number of detected bounding boxes: 2\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[145, 143, 135],\n",
      "        [144, 142, 134],\n",
      "        [144, 142, 134],\n",
      "        ...,\n",
      "        [120, 122, 116],\n",
      "        [122, 124, 118],\n",
      "        [128, 130, 124]],\n",
      "\n",
      "       [[143, 141, 133],\n",
      "        [143, 141, 133],\n",
      "        [143, 141, 133],\n",
      "        ...,\n",
      "        [119, 121, 115],\n",
      "        [122, 124, 118],\n",
      "        [128, 130, 124]],\n",
      "\n",
      "       [[143, 141, 133],\n",
      "        [144, 142, 134],\n",
      "        [144, 142, 134],\n",
      "        ...,\n",
      "        [121, 123, 117],\n",
      "        [122, 124, 118],\n",
      "        [126, 128, 122]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[151, 149, 141],\n",
      "        [146, 144, 136],\n",
      "        [145, 143, 135],\n",
      "        ...,\n",
      "        [135, 134, 130],\n",
      "        [139, 138, 134],\n",
      "        [135, 134, 130]],\n",
      "\n",
      "       [[142, 142, 136],\n",
      "        [140, 140, 134],\n",
      "        [142, 142, 136],\n",
      "        ...,\n",
      "        [141, 140, 136],\n",
      "        [139, 138, 134],\n",
      "        [138, 137, 133]],\n",
      "\n",
      "       [[139, 139, 133],\n",
      "        [147, 147, 141],\n",
      "        [162, 162, 156],\n",
      "        ...,\n",
      "        [149, 148, 144],\n",
      "        [132, 131, 127],\n",
      "        [134, 133, 129]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_16_13_04_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.5343894958496094, 'inference': 24.01900291442871, 'postprocess': 4.568338394165039}\n",
      "Detected Classes: {2}\n",
      "Number of detected bounding boxes: 3\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[147, 147, 141],\n",
      "        [148, 148, 142],\n",
      "        [147, 147, 141],\n",
      "        ...,\n",
      "        [128, 130, 124],\n",
      "        [131, 133, 127],\n",
      "        [125, 127, 121]],\n",
      "\n",
      "       [[146, 146, 140],\n",
      "        [152, 152, 146],\n",
      "        [147, 147, 141],\n",
      "        ...,\n",
      "        [127, 129, 123],\n",
      "        [132, 134, 128],\n",
      "        [127, 129, 123]],\n",
      "\n",
      "       [[140, 140, 134],\n",
      "        [151, 151, 145],\n",
      "        [145, 145, 139],\n",
      "        ...,\n",
      "        [123, 125, 119],\n",
      "        [127, 129, 123],\n",
      "        [122, 124, 118]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[146, 146, 140],\n",
      "        [142, 142, 136],\n",
      "        [144, 144, 138],\n",
      "        ...,\n",
      "        [138, 137, 133],\n",
      "        [141, 140, 136],\n",
      "        [140, 139, 135]],\n",
      "\n",
      "       [[134, 135, 131],\n",
      "        [136, 137, 133],\n",
      "        [142, 143, 139],\n",
      "        ...,\n",
      "        [138, 137, 133],\n",
      "        [141, 140, 136],\n",
      "        [133, 132, 128]],\n",
      "\n",
      "       [[144, 145, 141],\n",
      "        [152, 153, 149],\n",
      "        [162, 163, 159],\n",
      "        ...,\n",
      "        [164, 163, 159],\n",
      "        [159, 158, 154],\n",
      "        [140, 139, 135]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_16_13_27_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.396106719970703, 'inference': 14.63627815246582, 'postprocess': 2.7403831481933594}\n",
      "Detected Classes: {2}\n",
      "Number of detected bounding boxes: 3\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[145, 145, 139],\n",
      "        [147, 147, 141],\n",
      "        [141, 141, 135],\n",
      "        ...,\n",
      "        [125, 123, 115],\n",
      "        [126, 124, 116],\n",
      "        [128, 126, 118]],\n",
      "\n",
      "       [[146, 146, 140],\n",
      "        [140, 140, 134],\n",
      "        [130, 130, 124],\n",
      "        ...,\n",
      "        [126, 124, 116],\n",
      "        [127, 125, 117],\n",
      "        [127, 125, 117]],\n",
      "\n",
      "       [[144, 144, 138],\n",
      "        [142, 142, 136],\n",
      "        [137, 137, 131],\n",
      "        ...,\n",
      "        [127, 125, 117],\n",
      "        [126, 124, 116],\n",
      "        [124, 122, 114]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[139, 139, 133],\n",
      "        [136, 136, 130],\n",
      "        [140, 140, 134],\n",
      "        ...,\n",
      "        [135, 136, 132],\n",
      "        [139, 140, 136],\n",
      "        [126, 127, 123]],\n",
      "\n",
      "       [[140, 141, 137],\n",
      "        [132, 133, 129],\n",
      "        [139, 140, 136],\n",
      "        ...,\n",
      "        [137, 138, 136],\n",
      "        [135, 136, 134],\n",
      "        [128, 129, 127]],\n",
      "\n",
      "       [[132, 133, 129],\n",
      "        [133, 134, 130],\n",
      "        [159, 160, 156],\n",
      "        ...,\n",
      "        [155, 156, 154],\n",
      "        [151, 152, 150],\n",
      "        [159, 160, 158]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_16_13_53_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.424001693725586, 'inference': 12.999296188354492, 'postprocess': 2.4404525756835938}\n",
      "Detected Classes: {2}\n",
      "Number of detected bounding boxes: 3\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[177, 179, 167],\n",
      "        [184, 186, 174],\n",
      "        [176, 178, 166],\n",
      "        ...,\n",
      "        [123, 122, 118],\n",
      "        [119, 118, 114],\n",
      "        [122, 121, 117]],\n",
      "\n",
      "       [[173, 175, 163],\n",
      "        [176, 178, 166],\n",
      "        [177, 179, 167],\n",
      "        ...,\n",
      "        [123, 122, 118],\n",
      "        [120, 119, 115],\n",
      "        [121, 120, 116]],\n",
      "\n",
      "       [[172, 175, 160],\n",
      "        [171, 174, 159],\n",
      "        [185, 188, 173],\n",
      "        ...,\n",
      "        [123, 122, 118],\n",
      "        [122, 121, 117],\n",
      "        [120, 119, 115]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[132, 132, 126],\n",
      "        [131, 131, 125],\n",
      "        [130, 130, 124],\n",
      "        ...,\n",
      "        [128, 129, 127],\n",
      "        [126, 127, 125],\n",
      "        [125, 126, 124]],\n",
      "\n",
      "       [[135, 134, 130],\n",
      "        [131, 130, 126],\n",
      "        [128, 127, 123],\n",
      "        ...,\n",
      "        [130, 131, 129],\n",
      "        [126, 127, 125],\n",
      "        [129, 130, 128]],\n",
      "\n",
      "       [[136, 135, 131],\n",
      "        [145, 144, 140],\n",
      "        [156, 155, 151],\n",
      "        ...,\n",
      "        [157, 158, 156],\n",
      "        [150, 151, 149],\n",
      "        [154, 155, 153]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_16_18_23_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 3.0670166015625, 'inference': 12.284040451049805, 'postprocess': 2.294778823852539}\n",
      "Detected Classes: {4}\n",
      "Number of detected bounding boxes: 1\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[138, 138, 132],\n",
      "        [139, 139, 133],\n",
      "        [138, 138, 132],\n",
      "        ...,\n",
      "        [124, 120, 115],\n",
      "        [124, 120, 115],\n",
      "        [126, 122, 117]],\n",
      "\n",
      "       [[139, 139, 133],\n",
      "        [140, 140, 134],\n",
      "        [139, 139, 133],\n",
      "        ...,\n",
      "        [126, 122, 117],\n",
      "        [126, 122, 117],\n",
      "        [130, 126, 121]],\n",
      "\n",
      "       [[139, 139, 133],\n",
      "        [140, 140, 134],\n",
      "        [140, 140, 134],\n",
      "        ...,\n",
      "        [131, 128, 123],\n",
      "        [120, 116, 111],\n",
      "        [112, 108, 103]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[143, 143, 137],\n",
      "        [143, 143, 137],\n",
      "        [138, 138, 132],\n",
      "        ...,\n",
      "        [129, 128, 124],\n",
      "        [130, 129, 125],\n",
      "        [130, 129, 125]],\n",
      "\n",
      "       [[135, 134, 130],\n",
      "        [140, 139, 135],\n",
      "        [137, 136, 132],\n",
      "        ...,\n",
      "        [128, 126, 125],\n",
      "        [122, 120, 119],\n",
      "        [118, 116, 115]],\n",
      "\n",
      "       [[137, 136, 132],\n",
      "        [153, 152, 148],\n",
      "        [161, 160, 156],\n",
      "        ...,\n",
      "        [156, 154, 153],\n",
      "        [156, 154, 153],\n",
      "        [157, 155, 154]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_16_18_54_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 1.9562244415283203, 'inference': 15.339851379394531, 'postprocess': 1.8150806427001953}\n",
      "Detected Classes: {4}\n",
      "Number of detected bounding boxes: 2\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[140, 141, 131],\n",
      "        [147, 148, 138],\n",
      "        [142, 143, 133],\n",
      "        ...,\n",
      "        [128, 128, 122],\n",
      "        [132, 132, 126],\n",
      "        [130, 130, 124]],\n",
      "\n",
      "       [[138, 139, 129],\n",
      "        [144, 145, 135],\n",
      "        [141, 142, 132],\n",
      "        ...,\n",
      "        [128, 128, 122],\n",
      "        [130, 130, 124],\n",
      "        [126, 126, 120]],\n",
      "\n",
      "       [[138, 139, 130],\n",
      "        [143, 144, 135],\n",
      "        [140, 141, 132],\n",
      "        ...,\n",
      "        [127, 127, 121],\n",
      "        [129, 129, 123],\n",
      "        [124, 124, 118]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[140, 140, 134],\n",
      "        [135, 135, 129],\n",
      "        [133, 133, 127],\n",
      "        ...,\n",
      "        [136, 135, 131],\n",
      "        [137, 136, 132],\n",
      "        [139, 138, 134]],\n",
      "\n",
      "       [[140, 141, 137],\n",
      "        [132, 133, 129],\n",
      "        [125, 126, 122],\n",
      "        ...,\n",
      "        [132, 131, 127],\n",
      "        [130, 129, 125],\n",
      "        [131, 130, 126]],\n",
      "\n",
      "       [[127, 128, 124],\n",
      "        [153, 154, 150],\n",
      "        [163, 164, 160],\n",
      "        ...,\n",
      "        [146, 145, 141],\n",
      "        [146, 145, 141],\n",
      "        [155, 154, 150]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_16_19_59_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.016305923461914, 'inference': 13.436317443847656, 'postprocess': 1.9211769104003906}\n",
      "Detected Classes: {4}\n",
      "Number of detected bounding boxes: 3\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[140, 140, 134],\n",
      "        [141, 141, 135],\n",
      "        [143, 143, 137],\n",
      "        ...,\n",
      "        [127, 126, 122],\n",
      "        [128, 128, 122],\n",
      "        [126, 126, 120]],\n",
      "\n",
      "       [[141, 141, 135],\n",
      "        [142, 142, 136],\n",
      "        [144, 144, 138],\n",
      "        ...,\n",
      "        [127, 126, 122],\n",
      "        [127, 127, 121],\n",
      "        [125, 125, 119]],\n",
      "\n",
      "       [[139, 139, 133],\n",
      "        [140, 140, 134],\n",
      "        [144, 144, 138],\n",
      "        ...,\n",
      "        [126, 125, 121],\n",
      "        [127, 126, 122],\n",
      "        [126, 125, 121]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[139, 136, 131],\n",
      "        [139, 136, 131],\n",
      "        [140, 137, 132],\n",
      "        ...,\n",
      "        [134, 134, 128],\n",
      "        [138, 138, 132],\n",
      "        [135, 135, 129]],\n",
      "\n",
      "       [[142, 142, 136],\n",
      "        [140, 140, 134],\n",
      "        [138, 138, 132],\n",
      "        ...,\n",
      "        [139, 138, 134],\n",
      "        [143, 142, 138],\n",
      "        [130, 129, 125]],\n",
      "\n",
      "       [[144, 144, 138],\n",
      "        [151, 151, 145],\n",
      "        [161, 161, 155],\n",
      "        ...,\n",
      "        [154, 153, 149],\n",
      "        [155, 154, 150],\n",
      "        [134, 133, 129]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_16_21_50_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.285003662109375, 'inference': 13.808250427246094, 'postprocess': 1.9392967224121094}\n",
      "Detected Classes: {4}\n",
      "Number of detected bounding boxes: 4\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[137, 137, 131],\n",
      "        [142, 142, 136],\n",
      "        [139, 139, 133],\n",
      "        ...,\n",
      "        [129, 129, 123],\n",
      "        [130, 130, 124],\n",
      "        [124, 124, 118]],\n",
      "\n",
      "       [[142, 142, 136],\n",
      "        [146, 146, 140],\n",
      "        [141, 141, 135],\n",
      "        ...,\n",
      "        [129, 129, 123],\n",
      "        [130, 130, 124],\n",
      "        [125, 125, 119]],\n",
      "\n",
      "       [[144, 144, 138],\n",
      "        [147, 147, 141],\n",
      "        [142, 142, 136],\n",
      "        ...,\n",
      "        [128, 128, 122],\n",
      "        [128, 128, 122],\n",
      "        [127, 127, 121]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[139, 139, 133],\n",
      "        [143, 143, 137],\n",
      "        [136, 136, 130],\n",
      "        ...,\n",
      "        [135, 134, 130],\n",
      "        [135, 134, 130],\n",
      "        [135, 134, 130]],\n",
      "\n",
      "       [[136, 135, 131],\n",
      "        [142, 141, 137],\n",
      "        [135, 134, 130],\n",
      "        ...,\n",
      "        [131, 130, 126],\n",
      "        [133, 132, 128],\n",
      "        [134, 133, 129]],\n",
      "\n",
      "       [[142, 141, 137],\n",
      "        [158, 157, 153],\n",
      "        [161, 160, 156],\n",
      "        ...,\n",
      "        [157, 156, 152],\n",
      "        [160, 159, 155],\n",
      "        [162, 161, 157]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_16_22_20_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.4042129516601562, 'inference': 13.805627822875977, 'postprocess': 1.8925666809082031}\n",
      "Detected Classes: {4}\n",
      "Number of detected bounding boxes: 4\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[140, 140, 134],\n",
      "        [141, 141, 135],\n",
      "        [141, 141, 135],\n",
      "        ...,\n",
      "        [124, 123, 119],\n",
      "        [129, 128, 124],\n",
      "        [133, 132, 128]],\n",
      "\n",
      "       [[140, 140, 134],\n",
      "        [143, 143, 137],\n",
      "        [144, 144, 138],\n",
      "        ...,\n",
      "        [124, 123, 119],\n",
      "        [129, 128, 124],\n",
      "        [131, 130, 126]],\n",
      "\n",
      "       [[140, 140, 134],\n",
      "        [145, 145, 139],\n",
      "        [146, 146, 140],\n",
      "        ...,\n",
      "        [127, 126, 122],\n",
      "        [128, 127, 123],\n",
      "        [127, 126, 122]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[147, 147, 141],\n",
      "        [158, 158, 152],\n",
      "        [160, 160, 154],\n",
      "        ...,\n",
      "        [128, 129, 125],\n",
      "        [130, 131, 127],\n",
      "        [135, 136, 132]],\n",
      "\n",
      "       [[143, 142, 138],\n",
      "        [150, 149, 145],\n",
      "        [149, 148, 144],\n",
      "        ...,\n",
      "        [132, 133, 131],\n",
      "        [132, 133, 131],\n",
      "        [132, 133, 131]],\n",
      "\n",
      "       [[155, 154, 150],\n",
      "        [146, 145, 141],\n",
      "        [139, 138, 134],\n",
      "        ...,\n",
      "        [159, 160, 158],\n",
      "        [156, 157, 155],\n",
      "        [153, 154, 152]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_16_25_17_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.6099681854248047, 'inference': 13.269662857055664, 'postprocess': 2.2568702697753906}\n",
      "Detected Classes: {0, 1, 2, 3, 4}\n",
      "Number of detected bounding boxes: 5\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[144, 144, 138],\n",
      "        [143, 143, 137],\n",
      "        [141, 141, 135],\n",
      "        ...,\n",
      "        [124, 122, 121],\n",
      "        [125, 123, 122],\n",
      "        [126, 124, 123]],\n",
      "\n",
      "       [[141, 141, 135],\n",
      "        [141, 141, 135],\n",
      "        [140, 140, 134],\n",
      "        ...,\n",
      "        [126, 124, 123],\n",
      "        [126, 124, 123],\n",
      "        [127, 125, 124]],\n",
      "\n",
      "       [[143, 143, 137],\n",
      "        [142, 142, 136],\n",
      "        [141, 141, 135],\n",
      "        ...,\n",
      "        [128, 126, 125],\n",
      "        [127, 125, 124],\n",
      "        [127, 125, 124]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[152, 152, 146],\n",
      "        [152, 152, 146],\n",
      "        [152, 152, 146],\n",
      "        ...,\n",
      "        [117, 118, 116],\n",
      "        [116, 117, 115],\n",
      "        [116, 117, 115]],\n",
      "\n",
      "       [[149, 148, 144],\n",
      "        [148, 147, 143],\n",
      "        [147, 146, 142],\n",
      "        ...,\n",
      "        [118, 119, 117],\n",
      "        [118, 119, 117],\n",
      "        [117, 118, 116]],\n",
      "\n",
      "       [[155, 154, 150],\n",
      "        [161, 160, 156],\n",
      "        [169, 168, 164],\n",
      "        ...,\n",
      "        [145, 146, 144],\n",
      "        [144, 145, 143],\n",
      "        [143, 144, 142]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_16_27_08_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.3698806762695312, 'inference': 15.679597854614258, 'postprocess': 1.8935203552246094}\n",
      "Detected Classes: {1, 2, 3, 4}\n",
      "Number of detected bounding boxes: 4\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[135, 132, 127],\n",
      "        [132, 129, 124],\n",
      "        [131, 128, 123],\n",
      "        ...,\n",
      "        [125, 124, 120],\n",
      "        [125, 124, 120],\n",
      "        [125, 124, 120]],\n",
      "\n",
      "       [[134, 131, 126],\n",
      "        [132, 129, 124],\n",
      "        [130, 127, 122],\n",
      "        ...,\n",
      "        [124, 123, 119],\n",
      "        [125, 124, 120],\n",
      "        [127, 126, 122]],\n",
      "\n",
      "       [[133, 130, 125],\n",
      "        [131, 128, 123],\n",
      "        [130, 127, 122],\n",
      "        ...,\n",
      "        [125, 124, 120],\n",
      "        [127, 126, 122],\n",
      "        [129, 128, 124]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[145, 145, 139],\n",
      "        [144, 144, 138],\n",
      "        [145, 145, 139],\n",
      "        ...,\n",
      "        [114, 114, 114],\n",
      "        [115, 115, 115],\n",
      "        [117, 117, 117]],\n",
      "\n",
      "       [[152, 151, 147],\n",
      "        [147, 146, 142],\n",
      "        [145, 144, 140],\n",
      "        ...,\n",
      "        [116, 116, 116],\n",
      "        [116, 116, 116],\n",
      "        [114, 114, 114]],\n",
      "\n",
      "       [[151, 150, 146],\n",
      "        [157, 156, 152],\n",
      "        [165, 164, 160],\n",
      "        ...,\n",
      "        [145, 145, 145],\n",
      "        [143, 143, 143],\n",
      "        [140, 140, 140]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_16_27_27_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 3.9730072021484375, 'inference': 17.114639282226562, 'postprocess': 2.0732879638671875}\n",
      "Detected Classes: {0, 2, 4}\n",
      "Number of detected bounding boxes: 3\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[141, 141, 135],\n",
      "        [141, 141, 135],\n",
      "        [140, 140, 134],\n",
      "        ...,\n",
      "        [131, 132, 130],\n",
      "        [131, 132, 130],\n",
      "        [127, 128, 126]],\n",
      "\n",
      "       [[138, 138, 132],\n",
      "        [139, 139, 133],\n",
      "        [139, 139, 133],\n",
      "        ...,\n",
      "        [126, 127, 125],\n",
      "        [126, 127, 125],\n",
      "        [130, 131, 129]],\n",
      "\n",
      "       [[139, 139, 133],\n",
      "        [140, 140, 134],\n",
      "        [141, 141, 135],\n",
      "        ...,\n",
      "        [120, 121, 119],\n",
      "        [114, 115, 113],\n",
      "        [117, 118, 116]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[152, 149, 144],\n",
      "        [157, 154, 149],\n",
      "        [150, 147, 142],\n",
      "        ...,\n",
      "        [123, 126, 124],\n",
      "        [123, 126, 124],\n",
      "        [119, 122, 120]],\n",
      "\n",
      "       [[144, 141, 137],\n",
      "        [154, 151, 147],\n",
      "        [150, 147, 143],\n",
      "        ...,\n",
      "        [120, 123, 121],\n",
      "        [122, 125, 123],\n",
      "        [119, 122, 120]],\n",
      "\n",
      "       [[152, 149, 145],\n",
      "        [168, 165, 161],\n",
      "        [170, 167, 163],\n",
      "        ...,\n",
      "        [146, 149, 147],\n",
      "        [149, 152, 150],\n",
      "        [149, 152, 150]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_16_31_25_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.231597900390625, 'inference': 15.861034393310547, 'postprocess': 1.9822120666503906}\n",
      "Detected Classes: {0, 1, 2, 3, 4}\n",
      "Number of detected bounding boxes: 5\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "obb: None\n",
      "orig_img: array([[[132, 131, 127],\n",
      "        [132, 131, 127],\n",
      "        [131, 130, 126],\n",
      "        ...,\n",
      "        [128, 127, 123],\n",
      "        [129, 128, 124],\n",
      "        [129, 128, 124]],\n",
      "\n",
      "       [[132, 131, 127],\n",
      "        [131, 130, 126],\n",
      "        [131, 130, 126],\n",
      "        ...,\n",
      "        [127, 126, 122],\n",
      "        [128, 127, 123],\n",
      "        [128, 127, 123]],\n",
      "\n",
      "       [[131, 130, 126],\n",
      "        [131, 130, 126],\n",
      "        [131, 130, 126],\n",
      "        ...,\n",
      "        [125, 124, 120],\n",
      "        [126, 125, 121],\n",
      "        [126, 125, 121]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[146, 146, 140],\n",
      "        [147, 147, 141],\n",
      "        [146, 146, 140],\n",
      "        ...,\n",
      "        [117, 117, 117],\n",
      "        [118, 118, 118],\n",
      "        [118, 118, 118]],\n",
      "\n",
      "       [[141, 140, 136],\n",
      "        [144, 143, 139],\n",
      "        [145, 144, 140],\n",
      "        ...,\n",
      "        [116, 116, 116],\n",
      "        [116, 116, 116],\n",
      "        [116, 116, 116]],\n",
      "\n",
      "       [[146, 145, 141],\n",
      "        [156, 155, 151],\n",
      "        [165, 164, 160],\n",
      "        ...,\n",
      "        [144, 144, 144],\n",
      "        [144, 144, 144],\n",
      "        [144, 144, 144]]], dtype=uint8)\n",
      "orig_shape: (1080, 1920)\n",
      "path: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\ITC110_assignment_submission\\\\datasets\\\\datasetvs1_yolvo11\\\\..\\\\..\\\\data_for_testing_set1\\\\WIN_20250202_16_32_20_Pro.jpg'\n",
      "probs: None\n",
      "save_dir: 'c:\\\\Users\\\\wensi\\\\Desktop\\\\machine_learning\\\\object_detector\\\\runs\\\\detect\\\\predict'\n",
      "speed: {'preprocess': 2.4318695068359375, 'inference': 12.954950332641602, 'postprocess': 2.399444580078125}\n",
      "Detected Classes: {0, 2, 3, 4}\n",
      "Number of detected bounding boxes: 4\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "\n",
    "target_dir = os.path.join(HOME, 'datasets', 'datasetvs1_yolvo11')\n",
    "os.chdir(target_dir)\n",
    "\n",
    "img_path='../../data_for_testing_set1'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "output_dir = \"results_folder\"  # Choose your folder name\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#Testing Model 1 - Transfer Learning on Datasetvs1\n",
    "model_best = YOLO('model1/train_epoch30_yolov11n/weights/best.pt', task='detect')\n",
    "result = model_best(img_path, conf=0.5, iou=0.6)\n",
    "\n",
    "# Visualize the results\n",
    "for i, r in enumerate(result):\n",
    "    print(r)\n",
    "    # Plot results image\n",
    "    im_bgr = r.plot()  # BGR-order numpy array\n",
    "    im_rgb = Image.fromarray(im_bgr[..., ::-1])  # RGB-order PIL image\n",
    "    # Save results to disk\n",
    "    r.save(filename=os.path.join(output_dir, f\"results{i}.jpg\")) \n",
    "    #r.save(filename=f\"results{i}.jpg\")\n",
    "\n",
    "    # Get the list of detected classes\n",
    "    #The cls attribute usually stores the predicted class index of the detected object as a floating-point number.\n",
    "    detected_classes = [int(x.cls) for x in r.boxes]\n",
    "    unique_classes = set(detected_classes)\n",
    "\n",
    "    # Print the list of detected classes\n",
    "    print(\"Detected Classes:\", unique_classes)\n",
    "\n",
    "    # Count the number of detected bounding boxes\n",
    "    num_boxes = len(r.boxes) \n",
    "\n",
    "    print(f\"Number of detected bounding boxes: {num_boxes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f365767",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f58c20b",
   "metadata": {},
   "source": [
    "<h1>Application Function</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1ea6b7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3}\n",
      "{0: 'apple', 1: 'banana', 2: 'kiwi', 3: 'pear', 4: 'starfruit'}\n",
      "Total type 1\n",
      "Total amount: 3\n",
      "Only one type of fruit\n",
      "Type of fruit: pear\n"
     ]
    }
   ],
   "source": [
    "print (unique_classes)\n",
    "print (model_best.names)\n",
    "\n",
    "print (\"Total type\", len(unique_classes))\n",
    "print (\"Total amount:\", num_boxes)\n",
    "\n",
    "fruit_class_name = model_best.names\n",
    "unique_classes_list = list(unique_classes)\n",
    "if len(unique_classes) ==1:\n",
    "    print (\"Only one type of fruit\")\n",
    "    print (\"Type of fruit:\",fruit_class_name[unique_classes_list[0]])\n",
    "else:\n",
    "    print (\"More than one type of fruit\")\n",
    "    for x in range (len(unique_classes)):\n",
    "        print (\"List of Fruit:\",fruit_class_name[unique_classes_list[x]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
